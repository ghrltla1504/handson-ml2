{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4장 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 감사의 글\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 모델 훈련 작동과정 이해\n",
    "    * 디버깅, 에러 분석 등에 도움\n",
    "* 딥러닝의 신경망 이해, 구축, 훈련에 필요한 주제 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 개요\n",
    "\n",
    "* 선형 회귀 모델\n",
    "    * 수학적으로 모델 파라미터 구하기\n",
    "    * 경사하강법 적용 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 경사하강법 종류\n",
    "    * 배치 경사하강법\n",
    "    * 미니배치 경사하강법\n",
    "    * 확률적 경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 다항 회귀\n",
    "    * 비선형 모델 훈련법    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 곡선\n",
    "    * 과대적합 감지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 규제 선형 모델\n",
    "    * 과대적합 위험 감소시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 로지스틱 회귀와 소프트맥스 회귀\n",
    "    * 회귀 모델을 이용한 분류기로 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 모델 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 한 개의 특성 $x_1$을 사용하는 $i$번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)}$$\n",
    "\n",
    "- $n\\ge 1$ 개의 특성을 사용하는 $i$번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_n\\, x_n^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $n\\ge 1$ 개의 특성을 사용하는 $i$번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_n\\, x_n^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_n\\, x_n^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\hat y^{(i)}$: $i$ 번째 훈련 샘플에 대한 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $x_k^{(i)}$: $i$ 번째 훈련 샘플의 $k$ 번째 특성값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta_0$: 편향 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta_k\\, (k > 0$): $k$ 번째 특성에 대한 가중치 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 선형 회귀 모델의 행렬 연산 표기법\n",
    "\n",
    "$$\\hat {\\mathbf y}^T = h_\\theta (\\mathbf X) = \\theta^{T}\\, \\mathbf X_b^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 여기서 $\\mathbf{x}_b^{(i)} = \\begin{bmatrix} 1 & x_1^{(i)} & \\cdots & x_n^{(i)}\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 파이썬 넘파이 2차원 어레이 표현\n",
    "\n",
    "| 데이터 | 행렬 기호           |     수학 행렬 모양(shape) |  넘파이 어레이 모양 |\n",
    "|-------------:|-------------:|---------------:|------:|\n",
    "| 레이블, 예측값 | $\\mathbf y$, $\\hat{\\mathbf y}$  | $m \\times 1$ | (m, 1) |\n",
    "| 가중치 | $\\theta$      | $(n+1)\\times 1$ | (n+1, 1) |\n",
    "| 훈련 세트 | $\\mathbf X$   | $m\\times n$     | (m, n)   |\n",
    "| 훈련 센트(수정) | $\\mathbf X_b$ | $m\\times (n+1)$ | (m, n+1) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 비용함수: 평균 제곱 오차(MSE)\n",
    "\n",
    "* MSE를 활용한 선형 회귀 모델 성능 평가\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf X, h_\\theta) = \n",
    "\\frac 1 m \\sum_{i=0}^{m-1} (\\theta^{T}\\, \\mathbf (\\mathbf x_b^{(i)})^T - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 간단한 표현:\n",
    "\n",
    "$$\\mathrm{MSE}(\\theta) := \\mathrm{MSE}(\\mathbf X, h_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 목표: 비용함수 최소화\n",
    "\n",
    "* MSE를 아래와 같이 $\\theta$를 입력변수로 갖는 함수로 간주할 수 있음.\n",
    "\n",
    "$$\\mathrm{MSE}(\\theta) := \\mathrm{MSE}(\\mathbf X, h_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 이유: $\\mathbf X, m, \\mathbf x_b, y^{(i)}$은 모두 주어진 상수값들임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 목표: $\\mathrm{MSE}(\\theta)$가 최소가 되도록 하는 $\\theta$ 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방식 1: 정규방정식 또는 특이값 분해(SVD) 활용\n",
    "    * 드물지만 수학적으로 비용함수를 최소화하는 $\\theta$ 값을 직접 계산할 수 있는 경우 활용\n",
    "    * 계산복잡도가 $O(n^2)$ 이상인 행렬 연산을 수행해야 함. \n",
    "    * 따라서 특성 수($n$)이 큰 경우 메모리 관리 및 시간복잡도 문제때문에 비효율적임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 방식 2: 경사하강법\n",
    "    * 특성 수가 매우 크거나 훈련 샘플이 너무 많이 메모리에 모두 담을 수 없을 때 적합\n",
    "    * 일반적으로 선형 회귀 모델 훈련에 적용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 기본 아이디어\n",
    "\n",
    "* 훈련 세트를 이용한 훈련 과정 중에 가중치 등과 같은 **파라미터를 조금씩 반복적으로 조정하기**\n",
    "* 조정 기준: 비용 함수의 크기 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 관련 주요 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 최적 학습 모델\n",
    "\n",
    "* 비용함수를 최소화하는 또는 효용함수를 최대화하는 파라미터를 사용하는 모델\n",
    "\n",
    "* 예제: 선형 회귀 모델\n",
    "\n",
    "$$\\hat {\\mathbf y}^T = h_\\theta (\\mathbf X) = \\theta^{T}\\, \\mathbf X_b^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 파라미터\n",
    "\n",
    "* 예측값을 생성하는 함수로 구현되는 학습 모델에 사용되는 파라미터\n",
    "* 예제: 선형 회귀 모델에 사용되는 편향과 가중치 파라미터 \n",
    "\n",
    "$$\\theta = \\theta_0,\\theta_1, \\dots, \\theta_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 비용함수\n",
    "\n",
    "* 모델이 얼마나 나쁜지를 계산해주는 함수\n",
    "* 최적 학습 모델은 비용함수가 최소가 되도록 하는 모델\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE)\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf X, h_\\theta) = \n",
    "\\frac 1 m \\sum_{i=0}^{m-1} (\\theta^{T}\\, \\mathbf (\\mathbf x_b^{(i)})^T - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 전역 최솟값\n",
    "\n",
    "* 비용함수가 가질 수 있는 최솟값\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE) 함수가 갖는 최솟값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 그레이디언트 벡터\n",
    "\n",
    "* 다변수 함수의 미분값. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 여러 개의 값으로 이루어진 벡터로 계산됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (그레이디언트) 벡터는 방향과 크기에 대한 정보 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 그레이디언트가 가리키는 방향의 반대 방향으로 움직여야 가장 빠르게 전역 최솟값에 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 예제: 선형 회귀 MSE의 그레이디언트 벡터 $\\nabla_\\theta \\textrm{MSE}(\\theta)$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\textrm{MSE}(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_n} \\textrm{MSE}(\\theta)\n",
    "\\end{bmatrix} =\n",
    "\\frac{2}{m}\\, \\mathbf{X}_b^T\\, (\\mathbf{X}_b\\, \\theta - \\mathbf y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습률\n",
    "\n",
    "* 훈련 과정에서의 비용함수 파라미터 조정 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### 학습률과 모델 학습\n",
    "\n",
    "* 예제: 선형회귀 모델 파라미터 조정 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta$를 임의의 값으로 지정한 후 훈련 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 아래 단계를 $\\theta$가 특정 값에 지정된 오차범위 내로 수렴할 때까지 반복\n",
    "    1. (배치 크기로) 지정된 수의 훈련 샘플을 이용하여 학습.\n",
    "    2. 학습 후 $\\mathrm{MSE}(\\theta)$ 계산.\n",
    "    3. 이전 $\\theta$에서 $\\nabla_\\theta \\textrm{MSE}(\\theta)$과 학습률 $\\eta$를 곱한 값 빼기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\theta^{(\\text{new})} = \\theta^{(\\text{old})}\\, -\\, \\eta\\cdot \\nabla_\\theta \\textrm{MSE}(\\theta^{(\\text{old})})$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-01.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 학습률이 너무 작은 경우: 비용 함수가 전역 최소값에 너무 느리게 수렴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-02.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 학습률이 너무 큰 경우: 비용 함수가 수렴하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-03.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 시작점에 따라 지역 최솟값에 수렴할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 선형 회귀와 학습률\n",
    "    * 비용함수(MSE)가 볼록 함수. 즉, 지역 최솟값을 갖지 않음\n",
    "    * 따라서 학습률이 너무 크지 않으면 언젠가는 전역 최솟값에 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 하이퍼파라미터(hyperparameter)\n",
    "\n",
    "* 학습 모델을 지정할 때 사용되는 값\n",
    "\n",
    "* 예제: 학습률, 배치 크기, 에포크, 허용오차 등\n",
    "\n",
    "    * 에포크(epoch): 훈련 세트 전체를 대상으로 훈련하는 단계\n",
    "        * 에포크 수: 전체 훈렌 세트를 몇 번 반복 학습할 지 결정. \n",
    "    * 배치(batch) 크기: 파라미터를 업데이트하기 위해, 즉 그레이디언트 벡터를 계산하기 위해 필요한 훈련 샘플 수. \n",
    "    * 허용오차(tolerance): 비용함수의 그레이디언트 벡터의 크기가 허용오차보다 작아지면 학습 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 스텝(step)\n",
    "\n",
    "* 지정된 배치 크기의 샘플을 학습한 후에 파라미터를 조정하는 단계\n",
    "* 예제: 훈련 세트의 크기가 1,000이고 배치 크기가 10이면, \n",
    "    하나의 에포크 기간동안 총 100번의 스텝이 실행됨.\n",
    "* 스텝 크기 = (훈련 샘플 수) / (배치 크기)\n",
    "\n",
    "* 경우에 따라 배치 크기 대신에 스텝 크기가 하이퍼파라미터로 주어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 크기와 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 배치 경사 하강법\n",
    "\n",
    "* 배치 크기: 전체 훈련 샘플 수\n",
    "* 즉, 에포크마다 그레이디언트를 계산하여 파라미터 조정\n",
    "* __주의__: 여기서 사용되는 '배치'의 의미가 '배치 크기'의 '배치'와 다른 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 확률적 경사 하강법\n",
    "\n",
    "* 배치 크기: 1\n",
    "* 즉, 하나의 훈련 샘플을 학습할 때마다 그레이디언트를 계산해서 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 미니배치 경사 하강법\n",
    "\n",
    "* 배치 크기: 2에서 수백 사이\n",
    "* 최적 배치 크기: 경우에 따라 다름. 여러 논문이 32 이하 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 사이킷런은 배치 경사 하강법을 활용한 선형 회귀 미지원\n",
    "    * 책 176쪽, 표 4-1에서 사이킷런의 SGDRegressor가 배치 경사 하강법을 지원한다고 __잘못__ 명시됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "854ojx1fOtqk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 에포크와 허용오차\n",
    "\n",
    "* 에포크 수는 크게 설정한 후 허용오차를 지정하여 학습 시간 제한 필요\n",
    "\n",
    "* 이유: 포물선의 최솟점에 가까워질 수록 그레이디언트 벡터의 크기가 0에 수렴\n",
    "\n",
    "* 허용오차와 에포크 수는 서로 반비례의 관계\n",
    "    * 즉, 오차를 1/10로 줄이려면 에포크 수를 10배 늘려야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* 훈련 세트가 크면 그레이디언트를 계산하는 데에 많은 시간 필요\n",
    "* 아주 많은 데이터를 저장해야 하는 메모리 문제도 발생 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 장점\n",
    "\n",
    "* 매우 큰 훈련 세트를 다룰 수 있음\n",
    "    * 예를 들어, 외부 메모리(out-of-core) 학습을 활용할 수 있음\n",
    "* 학습 과정이 매우 빠름\n",
    "* 파라미터 조정이 불안정 할 수 있기 때문에 지역 최솟값에 상대적으로 덜 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* 학습 과정에서 파라미터의 동요가 심할 수 있음\n",
    "* 경우에 따라 전역 최솟값에 수렴하지 못하고 계속해서 발산할 가능성도 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습 스케줄\n",
    "\n",
    "* 요동치는 파라미터를 제어하기 위해 학습률을 학습 과정 동안 천천히 줄어들게 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 주의사항\n",
    "    * 학습률이 너무 빨리 줄어들면, 지역 최솟값에 갇힐 수 있음\n",
    "    * 학습률이 너무 느리게 줄어들면 전역 최솟값에 제대로 수렴하지 못하고 맴돌 수 있음\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 스케줄(learning schedule)\n",
    "    * 에포크, 훈련 샘플 수, 학습되는 샘플의 인덱스에 따른 학습률 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVXwxMY-SimN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사이킷런의 `SGDRegressor`\n",
    "\n",
    "* 경사 하강법 바로 지원\n",
    "\n",
    "* 사용되는 하이퍼파라미터\n",
    "  * `max_iter`: 에포크 수 제한\n",
    "  * `tol`: 허용 오차\n",
    "  * `eta0=0,1`: `SGDRegressor`가 사용하는 학습 스케줄 함수에 사용되는 매개 변수. 일종의 학습률.\n",
    "  * `penalty`: 규제 사용 여부 결정 (추후 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 미니배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 장점\n",
    "\n",
    "* 배치 크기를 어느 정도 크게 하면 확률적 경사 하강법(SGD) 보다 파라미터의 움직임이 덜 불규칙적이 됨\n",
    "* 반면에 배치 경사 하강법보다 빠르게 학습\n",
    "* 학습 스케줄을 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* SGD에 비해 지역 최솟값에 수렴할 위험도 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IvmA4ZvU3EJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-05.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 알고리즘 비교\n",
    "\n",
    "\n",
    "| 알고리즘   | 많은 샘플 수 | 외부 메모리 학습 | 많은 특성 수 | 하이퍼 파라미터 수 | 스케일 조정 | 사이킷런 지원 |\n",
    "|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| 정규방정식  | 빠름       | 지원 안됨      |  느림        | 0          | 불필요    | 지원 없음      |\n",
    "| SVD      | 빠름       | 지원 안됨      |  느림        | 0          | 불필요     | LinearRegression     |\n",
    "| 배치 GD   | 느림       | 지원 안됨      |  빠름        | 2          | 필요      | LogisticRegression      |\n",
    "| SGD      | 빠름       | 지원          |  빠름        | >= 2       | 필요      | SGDRegressor |\n",
    "| 미니배치 GD | 빠름       | 지원         |  빠름        | >=2        | 필요      | 지원 없음      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 선형 회귀를 이용하여 비선형 데이터를 학습하는 기법\n",
    "* 즉, 비선형 데이터를 학습하는 데 선형 모델 사용을 가능하게 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다항 회귀 기본 아이디어\n",
    "\n",
    "* 특성 조합 활용\n",
    "* 추가되는 특성은 기존의 특성값들의 거듭제곱, 특성값들 사이의 곱 등으로 이루어짐\n",
    "* 즉, 특성 변수들의 다항식을 조합 특성으로 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 vs. 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 선형 회귀\n",
    "\n",
    "* 1차 선형 모델: $\\hat y = \\theta_0 + \\theta_1\\, x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-06.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 다항 회귀\n",
    "\n",
    "* 2차 다항식 모델: $\\hat y = \\theta_0 + \\theta_1\\, x_1 + \\theta_2\\, x_1^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-07.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 사이킷런의 `PolynomialFeatures` 변환기\n",
    "\n",
    "* 주어진 특성들의 거듭제곱과 특성들 사이의 곱셈을 실행하여 특성을 추가하는 기능 제공\n",
    "\n",
    "* `degree=d` 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 예를 들어 $n=2, d=3$인 경우: $(x_1+x_2)^2 + (x_1+x_2)^3$의 항목에 해당하는 7개 특성 추가\n",
    "\n",
    "$$x_1^2,\\,\\, x_1 x_2,\\,\\, x_2^2,\\,\\, x_1^3,\\,\\, x_1^2 x_2,\\,\\, x_1 x_2^2,\\,\\, x_2^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 위 예제에서는 $n=1, d=2$ 이기에 $x_1^2$에 대한 특성 변수만 추가됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mt1fZDkqcCSM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 학습 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합/과대적합 판정\n",
    "\n",
    "* 예제: 선형 모델, 2차 다항 회귀 모델, 300차 다항 회귀 모델 비교\n",
    "\n",
    "* 다항 회귀 모델의 차수에 따라 훈련된 모델이 훈련 세트에 과소 또는 과대 적합할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-08.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 교차 검증 vs. 학습 곡선\n",
    "\n",
    "* 교차 검증(2장)\n",
    "    * 과소적합: 훈련 세트와 교차 검증 점수 모두 낮은 경우\n",
    "    * 과대적합: 훈련 세트에 대한 검증은 우수하지만 교차 검증 점수가 낮은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 곡선 살피기\n",
    "    * 학습 곡선: 훈련 세트와 검증 세트에 대한 모델 성능을 비교하는 그래프\n",
    "    * 학습 곡선의 모양에 따라 과소적합/과대적합 판정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-09.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 훈련 데이터에 대한 성능\n",
    "    * 훈련 세트가 커지면서 RMSE(평균 제곱근 오차)가 커짐\n",
    "    * 훈련 세트가 어느 정도 커지면 더 이상 RMSE가 변하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 검증 데이터에 대한 성능\n",
    "    * 검증 세트에 대한 성능이 훈련 세트에 대한 성능과 거의 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과대적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-10.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 훈련 데이터에 대한 성능\n",
    "    * 훈련 데이터에 대한 평균 제곱근 오차가 다른 모델보다 훨씬 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 검증 데이터에 대한 성능\n",
    "    * 훈련 데이터에 대한 성능과 차이가 크게 벌어진다. 즉, 훈련 데이터에 대한 성능이 훨씬 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 과대적합 모델 개선법\n",
    "\n",
    "* 검증 데이터에 대한 성능이 훈련 데이터에 대한 성능에 근접할 때까지 훈련 데이터의 크기 늘리기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 규제 선형 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 자유도와 규제\n",
    "\n",
    "* 자유도(degree of freedom): 학습 모델 결정에 영향을 주는 요소(특성)들의 수\n",
    "    * 단순 선형 회귀의 경우: 특성 수\n",
    "    * 다항 선형 회귀 경우: 차수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 규제(regularization): 자유도 제한\n",
    "    * 단순 선형 회귀 모델에 대한 규제: 가중치 역할 제한\n",
    "    * 다항 선형 회귀 모델에 대한 규제: 차수 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 가중치 역할 규제 선형 회귀 모델\n",
    "\n",
    "* 릿지 회귀\n",
    "* 라쏘 회귀\n",
    "* 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 규제 적용 주의사항\n",
    "\n",
    "규제항은 훈련 과정에만 사용된다. 테스트 과정에는 다른 기준으로 성능을 평가한다.\n",
    "\n",
    "* 훈련 과정: 비용 최소화 목표\n",
    "* 테스트 과정: 최종 목표에 따른 성능 평가\n",
    "    * 예제: 분류기의 경우 재현율/정밀도 기준으로 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 릿지 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 릿지 회귀의 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\, \\frac{1}{2} \\sum_{i=1}^{n}\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\alpha$(알파): 규제 강도를 지정하는 하이퍼파라미터\n",
    "\n",
    "* $\\alpha=0$: 단순 선형 회귀\n",
    "\n",
    "* $\\alpha$가 커질 수록 가중치의 역할이 줄어듦. \n",
    "    * 비용을 줄이기 위해 가중치를 작게 유지하는 방향으로 학습\n",
    "    \n",
    "* 주의사항: 훈련 세트에 대한 특성 스케일링 전처리 실행 후 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 라쏘 회귀의 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\, \\sum_{i=1}^{n}\\mid\\theta_i\\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\alpha$(알파)\n",
    "    * 하이퍼파라미터로 지정됨.\n",
    "    * 규제 강도 지정\n",
    "    * $\\alpha=0$이면 규제가 전혀 없는 기본 선형 회귀\n",
    "            \n",
    "* $\\theta_i$: 덜 중요한 특성을 무시하기 위해 $\\mid\\theta_i\\mid$가 0에 수렴하도록 학습 유도.\n",
    "\n",
    "* **주의**: $\\theta_0$은 규제하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 엘라스틱넷\n",
    "\n",
    "* 릿지 회귀와 라쏘 회귀를 절충한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 엘라스틱넷의 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + r\\, \\alpha \\, \\sum_{i=1}^{n}\\mid\\theta_i\\mid + \\,\\frac{1-r}{2}\\, \\alpha\\, \\sum_{i=1}^{n}\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $r$을 이용하여 릿지 규제와 라쏘 규제를 적절하게 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 규제 사용 방법\n",
    "\n",
    "* 대부분의 경우 약간이라도 규제 사용 추천\n",
    "* 릿지 규제가 기본\n",
    "* 유용한 속성이 많지 않다고 판단되는 경우 \n",
    "    * 라쏘 규제나 엘라스틱넷 활용 추천\n",
    "    * 불필요한 속성의 가중치를 0으로 만들기 때문\n",
    "* 특성 수가 훈련 샘플 수보다 크거나 특성 몇 개가 강하게 연관되어 있는 경우\n",
    "    * 라쏘 규제는 적절치 않음.\n",
    "    * 엘라스틱넷 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 조기 종료 기법\n",
    "\n",
    "* 반복 훈련 과정 중에 모델이 훈련 데이터에 점점 더 익숙해져서 과대적합 발생 가능\n",
    "* 따라서 반복 훈련을 적절한 시기에 종료해야 함\n",
    "* 반복훈련 종료 기준: 검증 데이터에 대한 손실이 줄어 들다가 다시 커지는 순간\n",
    "* 조기 종료: 검증 오차가 최소에 다다랐을 때 반복 훈련을 멈추게 하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-11.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 확률적 경사 하강법, 미니배치 경사 하강법의 경우 손실 곡선이 매끄럽지 않고 진동 발생 가능\n",
    "* 이런 경우에는 검증 오차가 한동안 최솟값보다 높게 유지될 때 반복 훈련을 멈추고\n",
    "    검증 오차가 최소였을 때의 모델 파라미터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.6 로지스틱 회귀와 소프트맥스 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "로지스틱 회귀와 소프트맥스 회귀를 이용하여 분류 모델 학습 가능\n",
    "\n",
    "* 이진 분류: 로지스틱 회귀 활용\n",
    "\n",
    "* 다중 클래스 분류: 소프트맥스 회귀 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 로지스틱 회귀와 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 로지스틱 회귀\n",
    "\n",
    "* 특성과 가중치의 곱한 값들을 더한 결과에 **시그모이드 함수**를 적용한 결과 이용\n",
    "\n",
    "* 로지스틱 회귀 모델에서 샘플 $\\mathbf x$에 대한 예측값\n",
    "\n",
    "$$\\hat p = h_\\theta (\\mathbf x) = \\sigma(\\theta^T \\, \\mathbf{x}_b^T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  시그모이드 함수\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-12.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 로지스틱 회귀 모델의 예측값\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}\\,\\, \\hat p < 0.5 \\\\\n",
    "1 & \\text{if}\\,\\, \\hat p \\ge 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta^T \\, \\mathbf{x}_b^T \\ge 0$ 인 경우: 양성 클래스(1)\n",
    "* $\\theta^T \\, \\mathbf{x}_b^T < 0$ 인 경우: 음성 클래스(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 로지스틱 회귀 모델의 비용함수\n",
    "\n",
    "* 로지스틱 회귀 모델을 경사하강법을 이용하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 비용함수: 로그 손실(log loss) 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=0}^{m-1}\\, [y^{(i)}\\, \\log(\\,\\hat p^{(i)}\\,) + (1-y^{(i)})\\, \\log(\\,1 - \\hat p^{(i)}\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 이 비용함수에 대해 경사 하강법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 로그 손실 함수 이해: 하나의 샘플에 대한 아래의 값의 의미 이해 중요\n",
    "\n",
    "$$\n",
    "- [y^{(i)}\\, \\log(\\,\\hat p^{(i)}\\,) + (1-y^{(i)})\\, \\log(\\,1 - \\hat p^{(i)}\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 틀린 예측을 하면 값이 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\log$ 함수 성질 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-13.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 결정 경계: 로지스틱 회귀 활용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사이킷런에서 제공하는 붓꽃 데이터셋 활용\n",
    "\n",
    "* 4개의 특성 사용\n",
    "    * 꽃받침 길이\n",
    "    * 꽃받침 너비\n",
    "    * 꽃잎 길이\n",
    "    * 꽃잎 너비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 샘플 타깃\n",
    "    * 0: Iris-Setosa\n",
    "    * 1: Iris-Versicolor\n",
    "    * 2: Iris-Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 꽃잎의 너비를 기준으로 Iris-Virginica 여부 판정하기\n",
    "\n",
    "* 결정경계: 약 1.6cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-14.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 꽃잎의 너비와 길이를 기준으로 Iris-Virginica 여부 판정하기\n",
    "\n",
    "* 결정경계: 검정 점선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-15.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 로지스틱 회귀 규제하기\n",
    "\n",
    "* 하이퍼파라미터 `penalty`와 `C` 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `penalty`\n",
    "    * `l1`, `l2`, `elasticnet` 세 개중에 하나 사용.\n",
    "    * 기본은 `l2`, 즉, $\\ell_2$ 규제를 사용하는 릿지 규제.\n",
    "    * `elasticnet`을 선택한 경우 `l1_ration` 옵션 값을 지정해서 함께 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `C`\n",
    "    * 릿지 규제 정도를 지정하는 $\\alpha$의 역수에 해당. \n",
    "    * 따라서 0에 가까울 수록 강한 규제 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 소프트맥스(softmax) 회귀\n",
    "\n",
    "* 로지스틱 회귀 모델을 일반화하여 다중 클래스 분류를 지원하도록 한 회귀 모델\n",
    "* **다항 로지스틱 회귀** 라고도 불림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 소프트맥스 회귀 학습 아이디어\n",
    "\n",
    "* 샘플 $\\mathbf x$이 주어졌을 때 각각의 분류 클래스 $k$ dp 대한 점수 $s_k(\\mathbf x)$ 계산\n",
    "\n",
    "$$\n",
    "s_k(\\mathbf x) = (\\theta^{(k)})^T\\, \\mathbf{x}_b^T\n",
    "$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 소프트맥스 함수를 이용하여 각 클래스 $k$에 속할 확률 $\\hat p_k$ 계산\n",
    "\n",
    "$$\n",
    "\\hat p_k = \n",
    "\\frac{\\exp(s_k(\\mathbf x))}{\\sum_{j=0}^{K-1}\\exp(s_j(\\mathbf x))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 추정 확률이 가장 높은 클래스 선택\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\mathrm{argmax}_k s_k(\\mathbf x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 주의사항\n",
    "\n",
    "* 소프트맥스 회귀는 다중 출력 분류 지원 못함.\n",
    "* 예를 들어, 하나의 사진에서 여러 사람의 얼굴 인식 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 소프트맥스 회귀 비용함수\n",
    "\n",
    "* 각 분류 클래스 $k$에 대한 적절한 가중치 벡터 $\\theta_k$를 학습해 나가야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 비용함수: 크로스 엔트로피 비용 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\Theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=0}^{m-1}\\sum_{k=0}^{K-1} y^{(i)}_k\\, \\log(\\hat{p}_k^{(i)})\n",
    "$$\n",
    "\n",
    "* 이 비용함수에 대해 경사 하강법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 참조\n",
    "\n",
    "* $K=2$이면 로지스틱 회귀의 로그 손실 함수와 정확하게 일치한다.\n",
    "\n",
    "* 주어진 샘플의 타깃 클래스를 제대로 예측할 경우 높은 확률값 계산\n",
    "\n",
    "* 크로스 엔트로피 개념은 정보 이론에서 유래하였다. (자세한 설명은 생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 멀티 클래스 분류: 소프트맥스 회귀 활용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 붓꽃을 세 개의 클래스로 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 사이킷런의 `LogisticRegression` 예측기 활용\n",
    "    * `multi_class` 하이퍼파라미터 값을 `multinomial`로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 꽃잎의 너비와 길이를 기준으로 붓꽃 클래스 분류\n",
    "\n",
    "* 결정경계: 배경색으로 구분\n",
    "* 곡선: Iris-Versicolor 클래스에 속할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-16.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 예제: 꽃잎 길이 5cm, 너비 2cm 인 붓꽃에 대한 품종 클래스 추정\n",
    "    * 94.2%의 확률로 Iris-Virginica\n",
    "    * 또는 5.8%의 확률로 Iris-Versicolor"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "핸즈온머신러닝_4장.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
