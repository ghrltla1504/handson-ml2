{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4장 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 모델 훈련 작동과정 이해\n",
    "    * 디버깅, 에러 분석 등에 도움\n",
    "* 딥러닝의 신경망 이해, 구축, 훈련에 필요한 주제 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 개요\n",
    "\n",
    "* 선형 회귀 모델\n",
    "    * 수학적으로 모델 파라미터 구하기\n",
    "    * 경사하강법 적용 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 경사하강법 종류\n",
    "    * 배치 경사하강법\n",
    "    * 미니배치 경사하강법\n",
    "    * 확률적 경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 다항 회귀\n",
    "    * 비선형 모델 훈련법    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 곡선\n",
    "    * 과대적합 감지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 규제 선형 모델\n",
    "    * 과대적합 위험 감소시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 로지스틱 회귀와 소프트맥스 회귀\n",
    "    * 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 모델 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 한 개의 특성 $x_1$을 사용하는 $i$번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $n\\ge 1$ 개의 특성을 사용하는 $i$번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_n\\, x_n^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\hat y^{(i)}$: $i$ 번째 훈련 샘플에 대한 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $x_k^{(i)}$: $i$ 번째 훈련 샘플의 $k$ 번째 특성값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta_0$: 편향 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta_k\\, (k > 0$): $k$ 번째 특성에 대한 가중치 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 선형 회귀 모델의 행렬 연산 표기법\n",
    "\n",
    "$$\\hat {\\mathbf y}^T = h_\\theta (\\mathbf X) = \\theta^{T}\\, \\mathbf X_b^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 여기서 $\\mathbf{x}_b^{(i)} = \\begin{bmatrix} 1 & x_1^{(i)} & \\cdots & x_n^{(i)}\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 파이썬 넘파이 2차원 어레이 표현\n",
    "\n",
    "| 데이터 | 행렬 기호           |     수학 행렬 모양(shape) |  넘파이 어레이 모양 |\n",
    "|-------------:|-------------:|---------------|------:|\n",
    "| 레이블, 예측값 | $\\mathbf y$, $\\hat{\\mathbf y}$  | $m \\times 1$ | (m, 1) |\n",
    "| 가중치 | $\\theta$      | $(n+1)\\times 1$ | (n+1, 1) |\n",
    "| 훈련 세트 | $\\mathbf X$   | $m\\times n$     | (m, n)   |\n",
    "| 훈련 센트(수정) | $\\mathbf X_b$ | $m\\times (n+1)$ | (m, n+1) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 비용함수: 평균 제곱 오차(MSE)\n",
    "\n",
    "* MSE를 활용한 선형 회귀 모델 성능 평가\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf X, h_\\theta) = \n",
    "\\frac 1 m \\sum_{i=0}^{m-1} (\\theta^{T}\\, \\mathbf (\\mathbf x_b^{(i)})^T - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 간단한 표현:\n",
    "\n",
    "$$\\mathrm{MSE}(\\theta) := \\mathrm{MSE}(\\mathbf X, h_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 목표: 비용함수 최소화\n",
    "\n",
    "* MSE를 아래와 같이 $\\theta$를 입력변수로 갖는 함수로 간주할 수 있음.\n",
    "\n",
    "$$\\mathrm{MSE}(\\theta) := \\mathrm{MSE}(\\mathbf X, h_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 이유: $\\mathbf X, m, \\mathbf x_b, y^{(i)}$은 모두 주어진 상수값들임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 목표: $\\mathrm{MSE}(\\theta)$가 최소가 되도록 하는 $\\theta$ 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방식 1: 정규방정식 또는 특이값 분해(SVD) 활용\n",
    "    * 드물지만 수학적으로 비용함수를 최소화하는 $\\theta$ 값을 직접 계산할 수 있는 활용\n",
    "    * 계산복잡도가 $O(n^2)$ 이상인 행렬 연산을 수행해야 함. \n",
    "    * 따라서 특성 수($n$)이 큰 경우 메모리 관리 및 시간복잡도 문제때문에 비효율적임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 방식 2: 경사하강법\n",
    "    * 특성 수가 매우 크거나 훈련 샘플이 너무 많이 메모리에 모두 담을 수 없을 때 적합\n",
    "    * 일반적으로 선형 회귀 모델 훈련에 적용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 기본 아이디어\n",
    "\n",
    "* 훈련 세트를 이용한 훈련 과정 중에 가중치 등과 같은 **파라미터를 조금씩 반복적으로 조정하기**\n",
    "* 조정 기준: 비용 함수의 크기 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 관련 주요 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 최적 학습 모델\n",
    "\n",
    "* 비용함수를 최소화하는 또는 효용함수를 최대화하는 파라미터를 사용하는 모델\n",
    "\n",
    "* 예제: 선형 회귀 모델\n",
    "\n",
    "$$\\hat {\\mathbf y}^T = h_\\theta (\\mathbf X) = \\theta^{T}\\, \\mathbf X_b^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 파라미터\n",
    "\n",
    "* 예측값을 생성하는 함수로 구현되는 학습 모델에 사용되는 파라미터\n",
    "* 예제: 선형 회귀 모델에 사용되는 편향과 가중치 파라미터 \n",
    "\n",
    "$$\\theta = \\theta_0,\\theta_1, \\dots, \\theta_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 비용 함수\n",
    "\n",
    "* 최적 학습 모델은 비용 함수가 최소가 되도록 하는 모델\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE)\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf X, h_\\theta) = \n",
    "\\frac 1 m \\sum_{i=0}^{m-1} (\\theta^{T}\\, \\mathbf (\\mathbf x_b^{(i)})^T - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 전역 최솟값\n",
    "\n",
    "* 비용 함수가 갖는 최소값\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE) 함수가 갖는 최솟값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 그레이디언트 벡터\n",
    "\n",
    "* 다변수 함수의 미분값. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 여러 개의 값으로 이루어진 벡터로 계산됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (그레이디언트) 벡터는 방향과 크기에 대한 정보 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 그레이디언트가 가리키는 방향의 반대 방향으로 움직여야 가장 빠르게 전역 최솟값에 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 예제: 선형 회귀 MSE의 그레이디언트 벡터 $\\nabla_\\theta \\textrm{MSE}(\\theta)$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\textrm{MSE}(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_n} \\textrm{MSE}(\\theta)\n",
    "\\end{bmatrix} =\n",
    "\\frac{2}{m}\\, \\mathbf{X}_b^T\\, (\\mathbf{X}_b\\, \\theta - \\mathbf y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습률\n",
    "\n",
    "* 훈련 과정에서의 비용함수 파라미터 조정 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### 학습률과 모델 학습\n",
    "\n",
    "* 예제: 선형회귀 모델 파라미터 조정 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta$를 임의의 값으로 지정한 후 훈련 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 아래 단계를 $\\theta$가 특정 값에 지정된 오차범위 내로 수렴할 때까지 반복\n",
    "    1. (배치 크기로) 지정된 수의 훈련 샘플을 이용하여 학습.\n",
    "    2. 학습 후 $\\mathrm{MSE}(\\theta)$ 계산.\n",
    "    3. 이전 $\\theta$에서 $\\nabla_\\theta \\textrm{MSE}(\\theta)$과 학습률 $\\eta$를 곱한 값 빼기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\theta^{(\\text{new})} = \\theta^{(\\text{old})}\\, -\\, \\eta\\cdot \\nabla_\\theta \\textrm{MSE}(\\theta^{(\\text{old})})$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-01.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 학습률이 너무 너무 작은 경우: 비용 함수가 전역 최소값에 너무 느리게 수렴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-02.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 학습률이 너무 큰 경우: 비용 함수가 수렴하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-03.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 시작점에 따라 지역 최솟값에 수렴할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 선형 회귀와 학습률\n",
    "    * 비용함수(MSE)가 볼록 함수. 즉, 지역 최솟값을 갖지 않음\n",
    "    * 따라서 학습률이 너무 크지 않으면 언젠가는 전역 최솟값에 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 하이퍼파라미터(hyperparameter)\n",
    "\n",
    "* 학습 모델을 지정할 때 사용되는 값\n",
    "\n",
    "* 예제: 학습률, 배치 크기, 에포크, 허용오차 등\n",
    "\n",
    "    * 에포크(epoch): 훈련 세트 전체를 대상으로 훈련하는 단계\n",
    "        * 에포크 수: 전체 훈렌 세트를 몇 번 반복 학습할 지 결정. \n",
    "    * 배치(batch) 크기: 파라미터를 업데이트하기 위해, 즉 그레이디언트 벡터를 계산하기 위해 필요한 훈련 샘플 수. \n",
    "    * 허용오차(tolerance): 비용함수의 그레이디언트 벡터의 크기가 허용오차보다 작아지면 학습 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 스텝(step)\n",
    "\n",
    "* 지정된 배치 크기의 샘플을 학습한 후에 파라미터를 조정하는 단계\n",
    "* 예제: 훈련 세트의 크기가 1,000이고 배치 크기가 10이면, \n",
    "    하나의 에포크 기간동안 총 100번의 스텝이 실행됨.\n",
    "* 스텝 크기 = (훈련 샘플 수) / (배치 크기)\n",
    "\n",
    "* 경우에 따라 배치 크기 대신에 스텝 크기가 하이퍼파라미터로 주어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 크기와 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 배치 경사 하강법\n",
    "\n",
    "* 배치 크기: 전체 훈련 샘플 수\n",
    "* 즉, 에포크마다 그레이디언트를 계산하여 파라미터 조정\n",
    "* __주의__: 여기서 사용되는 '배치'의 의미가 '배치 크기'의 '배치'와 다른 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 확률적 경사 하강법\n",
    "\n",
    "* 배치 크기: 1\n",
    "* 즉, 하나의 훈련 샘플을 학습할 때마다 그레이디언트를 계산해서 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 미니배치 경사 하강법\n",
    "\n",
    "* 배치 크기: 2에서 수백 사이\n",
    "* 최적 배치 크기: 경우에 따라 다름. 여러 논문이 32 이하 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 사이킷런은 배치 경사 하강법을 활용한 선형 회귀 미지원\n",
    "    * 책 176쪽, 표 4-1에서 사이킷런의 SGDRegressor가 배치 경사 하강법을 지원한다고 __잘못__ 명시됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "854ojx1fOtqk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 에포크와 허용오차\n",
    "\n",
    "* 에포크 수는 크게 설정한 후 허용오차를 지정하여 학습 시간 제한 필요\n",
    "\n",
    "* 이유: 포물선의 최솟점에 가까워질 수록 그레이디언트 벡터의 크기가 0에 수렴\n",
    "\n",
    "* 허용오차와 에포크 수는 서로 반비례의 관계\n",
    "    * 즉, 오차를 1/10로 줄이려면 에포크 수를 10배 늘려야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* 훈련 세트의 크면 그레이디언트를 계산하는 데에 많은 시간 필요\n",
    "* 아주 많은 데이터를 저장해야 하는 메모리 문제도 발생 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 장점\n",
    "\n",
    "* 매우 큰 훈련 세트를 다룰 수 있음\n",
    "    * 예를 들어, 외부 메모리(out-of-core) 학습을 활용할 수 있음\n",
    "* 학습 과정이 매우 빠름\n",
    "* 파라미터 조정이 불안정 할 수 있기 때문에 지역 최솟값에 상대적으로 덜 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* 학습 과정에서 파라미터의 동요가 심할 수 있음\n",
    "* 경우에 따라 전역 최솟값에 수렴하지 못하고 계속해서 발산할 가능성도 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습 스케줄\n",
    "\n",
    "* 요동치는 파라미터를 제어하기 위해 학습률을 학습 과정 동안 천천히 줄어들게 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 주의사항\n",
    "    * 학습률이 너무 빨리 줄어들면, 지역 최솟값에 갇힐 수 있음\n",
    "    * 학습률이 너무 느리게 줄어들면 전역 최솟값에 제대로 수렴하지 못하고 맴돌 수 있음\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 스케줄(learning schedule)\n",
    "    * 에포크, 훈련 샘플 수, 학습되는 샘플의 인덱스에 따른 학습률 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVXwxMY-SimN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사이킷런의 `SGDRegressor`\n",
    "\n",
    "* 경사 하강법 바로 지원\n",
    "\n",
    "* 사용되는 하이퍼파라미터\n",
    "  * `max_iter`: 에포크 수 제한\n",
    "  * `tol`: 허용 오차\n",
    "  * `eta0=0,1`: `SGDRegressor`가 사용하는 학습 스케줄 함수에 사용되는 매개 변수. 일종의 학습률.\n",
    "  * `penalty`: 규제 사용 여부 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 미니배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 장점\n",
    "\n",
    "* 배치 크기를 어느 정도 크게 하면 확률적 경사 하강법(SGD) 보다 파라미터의 움직임이 덜 불규칙적이 됨\n",
    "* 반면에 배치 경사 하강법보다 빠르게 학습\n",
    "* 학습 스케줄을 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 단점\n",
    "\n",
    "* SGD에 비해 지역 최솟값에 수렴할 위험도 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IvmA4ZvU3EJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-05.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 알고리즘 비교\n",
    "\n",
    "\n",
    "| 알고리즘   | 많은 샘플 수 | 외부 메모리 학습 | 많은 특성 수 | 하이퍼 파라미터 수 | 스케일 조정 | 사이킷런 지원 |\n",
    "|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| 정규방정식  | 빠름       | 지원 안됨      |  느림        | 0          | 불필요    | 지원 없음      |\n",
    "| SVD      | 빠름       | 지원 안됨      |  느림        | 0          | 불필요     | LinearRegression     |\n",
    "| 배치 GD   | 느림       | 지원 안됨      |  빠름        | 2          | 필요      | LogisticRegression      |\n",
    "| SGD      | 빠름       | 지원          |  빠름        | >= 2       | 필요      | SGDRegressor |\n",
    "| 미니배치 GD | 빠름       | 지원         |  빠름        | >=2        | 필요      | 지원 없음      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 선형 회귀를 이용하여 비선형 데이터를 학습하는 기법\n",
    "* 즉, 비선형 데이터를 학습하는 데 선형 모델 사용을 가능하게 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다항 회귀 기본 아이디어\n",
    "\n",
    "* 특성 조합 활용\n",
    "* 추가되는 특성은 기존의 특성값들의 거듭제곱, 특성값들 사이의 곱 등으로 이루어짐\n",
    "* 즉, 특성 변수들의 다항식을 조합 특성으로 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 vs. 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 선형 회귀\n",
    "\n",
    "<div align=\"center\"><img src=\"images/ch04/homl04-06.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 다항 회귀\n",
    "\n",
    "* 특성 변수들의 2차 다항식 활용\n",
    "\n",
    "<div align=\"center\"><img src=\"images/ch04/homl04-07.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 사이킷런의 `PolynomialFeatures` 변환기\n",
    "\n",
    "* 주어진 특성들의 거듭제곱과 특성들 사이의 곱셈을 실행하여 특성을 추가하는 기능 제공\n",
    "\n",
    "* 앞서 다항식의 차수를 2차로 지정한 `degree=2` 하이퍼파라미터 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mt1fZDkqcCSM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 학습 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합/과대적합 판정\n",
    "\n",
    "* 예제: 선형 모델, 2차 다항 회귀 모델, 300차 다항 회귀 모델 비교\n",
    "\n",
    "* 다항 회귀 모델의 차수에 따라 훈련된 모델이 훈련 세트에 과소 또는 과대 적합할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-08.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 교차 검증 vs. 학습 곡선\n",
    "\n",
    "* 교차 검증(2장)\n",
    "    * 과소적합: 훈련 세트와 교차 검증 점수 모두 낮은 경우\n",
    "    * 과대적합: 훈련 세트에 대한 검증은 우수하지만 교차 검증 점수가 낮은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 학습 곡선 살피기\n",
    "    * 학습 속선: 훈련 세트와 검증 세트에 대한 모델 성능을 비교하는 그래프\n",
    "    * 학습 곡선의 모양에 따라 과소적합/과대적합 판정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-09.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 훈련 데이터에 대한 성능\n",
    "    * 훈련 세트가 커지면서 RMSE(평균 제곱근 오차)가 커짐\n",
    "    * 훈련 세트가 어느 정도 커지면 더 이상 RMSE가 변하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 검증 데이터에 대한 성능\n",
    "    * 검증 세트에 대한 성능이 훈련 세트에 대한 성능과 거의 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과대적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-10.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 훈련 데이터에 대한 성능\n",
    "    * 훈련 데이터에 대한 평균 제곱근 오차가 다른 모델보다 훨씬 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 검증 데이터에 대한 성능\n",
    "    * 훈련 데이터에 대한 성능과 차이가 크게 벌어진다. 즉, 훈련 데이터에 대한 성능이 훨씬 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 과대적합 모델 개선법\n",
    "\n",
    "* 검증 데이터에 대한 성능이 훈련 데이터에 대한 성능에 근접할 때가지 훈련 데이터의 크기 늘리기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 규제 선형 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 자유도와 규제\n",
    "\n",
    "* 자유도(degree of freedom): 학습 모델 결정에 영향을 주는 요소(특성)들의 수\n",
    "    * 단순 선형 회귀의 경우: 특성 수\n",
    "    * 다항 선형 회귀 경우: 차수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 규제(regularization): 자유도 제한\n",
    "    * 단순 선형 회귀 모델에 대한 규제: 가중치 역할 제한\n",
    "    * 다항 선형 회귀 모델에 대한 규제: 차수 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 가중치 역할 제한 선형 회귀 모델\n",
    "\n",
    "* 릿지 회귀\n",
    "* 라쏘 회귀\n",
    "* 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 릿지 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 릿지 회귀의 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\, \\frac{1}{2} \\sum_{i=1}^{n}\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\alpha$(알파): 규제 강도를 지정하는 하이퍼파라미터\n",
    "\n",
    "* $\\alpha=0$: 단순 선형 회귀\n",
    "\n",
    "* $\\alpha$가 커질 수록 가중치의 역할이 줄어듦. \n",
    "    * 비용을 줄이기 위해 가중치를 작게 유지하는 방향으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "핸즈온머신러닝_4장.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
