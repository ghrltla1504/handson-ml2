{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 18장 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 감사의 글\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 머신러닝 분야에서 가장 흥미로우며 가장 오래된 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 게임, 기계 제어 등 매우 다양한 애플리케이션에서 활용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 강화학습(Reinforcement Learning) 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 심층 강화학습의 주요 기법 두 가지\n",
    "    * 정책 그레이디언트(policy gradients)\n",
    "    * 심층 Q-네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 실전 예제 1: 움직이는 카드(cart)에서 막대 균형잡기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 마르코프 결정과정(Markov decision processes, MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실전 예제 2: 브레이크아웃(Bradkout)이라는 아타리(Atari) 게임 플레이어 훈련시키기\n",
    "    * TF-Agents 라이브러리 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1절 보상 최적화 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 소프트웨어 에이전트(agent)가 주어진 환경에서 관측(observation) 후 행동(action)을 취하는 행위 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행위 결과에 따라 양(positive) 또는 음(negative)의 보상을 받음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목표: 최대한의 (양의) 보상과 최소한의 (음의) 보상 받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 에이전트 학습 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 주요 용어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-01.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 1: 로봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 로봇 제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 실제 세상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 카메라, 터치 센서 등을 이용하여 환경 관찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 모터를 구동하기 위해 시그널 전송"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 목적지에 도착할 때 양의 보상, 시간을 낭비하거나 잘못된 방향으로 향할 때 음의 보상 받음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 2: 미스 팩맨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 미스 팩맨 제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 아타리 게임 시뮬레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 스크린샷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 가능한 아홉 가지 조이스틱 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 게임 점수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 3: 바둑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 미스 팩맨과 유사하게 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 4: 온도조절기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 온도제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 주위 온도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 온도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 온도 조절\n",
    "    * 사람의 요구를 예측하도록 학습된 결과에 따라 행동 취함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 에너지를 절약하면 양의 보상, 사람이 온도를 조작할 필요가 발생하면 음의 보상을 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 5: 주식 자동매매 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 자동매매 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 주식시장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 주식시장 가격"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 매초 얼마나 사고팔아야 할지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 금전적 이익과 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 활용 영역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자율주행 자동차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 추천 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹페이지 상에 광고배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지 분류시스템 포커싱(주의집중) 영역 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 보상의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 양의 보상: 기쁨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 음의 보상: 아픔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 양 또는 음의 보상이 전혀 없을 수도 있음\n",
    "    * 미로 게임 에이전트는 타임스텝마다 음의 보상을 받기에 빠르게 탈출하도록 학습함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2절 정책 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 에이전트(agent)가 행동을 결정하기 위해 사용하는 알고리즘\n",
    "    * 입력값: 관측\n",
    "    * 출력: 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 입력값이 필요없을 수도 있음\n",
    "    * 예제: 30분 동안 수집한 먼지 양을 보상으로 받는 로봇진공청소기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-02.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 무작위성이 포함된 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제: 로봇진공청소기\n",
    "    * 매 초마다 지정된 확률 p 만큼 전진.\n",
    "    * 무작위적으로 (1-p) 의 확률로 왼쪽 또는 오른쪽으로 회전하기.\n",
    "    * 회전 각도는 -r 과 r 사이의 임의의 값.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책 파라미터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 파라미터**: 정책에 사용되는 변경가능한 파라미터\n",
    "    * 로봇진공청소기의 경우: p 와 r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 탐색**: 가장 성는이 좋은 정책 마라미터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 공간**: 정책 파라미터가 취할 수 있는 값들의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 정책 탐색 기법: 유전 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제\n",
    "    * 1세대: 정책 100개 랜덤하게 생성하여 사용해본 후 상위 20% 정책만 남김\n",
    "    * 2세대: 남겨진 20개을 정책을 각각 4개씩 약간의 무작위성을 추가하여 복사.\n",
    "    * 위 과정을 좋은 정책을 찾을 때까지 여러 세대에 걸쳐 반복.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주요 활용 예제: NEAT(NeuroEvolution of Augmenting Topologies) 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 정책 탐색 기법: 정책 그레이디언트(PG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경사하강법과 유사한 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책을 따른 결과인 보상이 최댓값을 갖도록 정책 파라미터들을 조금씩 변경하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상이 최댓값을 갖도록 하기에 **경사상승법** 이라고도 불림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: 로봇 진공청소기\n",
    "    * p의 값을 조금 크게 한 경과 30분 동안 더 많은 먼지를 수집할 경우 p의 값을 좀 더 키움.\n",
    "    그렇지 않으면 p의 값을 조금 줄임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PG에 대해 이번 장에서 자세히 다룰 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PG를 TF(텐서플로우)로 구현하려면 에이전트가 활동할 환경을 먼저 세팅해야 함.\n",
    "    여기서는 **Open AI Gym** 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3절 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 아타리 게임, 보드게임, 2D/3D 물리적 시물레이션 등을 위한 시뮬레이션 환경 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시뮬레이션 환경 내에서 에이전트에 대한 다양한 정책을 훈련하고 비교할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 설치 요령은 책과 코랩 노트북 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CartPole 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 카드 위에 놓인 막대가 넘어지지 않도록 오른쪽/왼쪽으로 가속할 수 있는 2D 시뮬레이션 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-03.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CartPole 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 환경 리셋하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole-v1 환경을 생성한 후 리셋하여 환경 초기화 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset() # 초기화된 환경 관측치 할당\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* CartPole-v1의 경우 `reset()` 메서드는 아래 모양의 초기화된 환경 관측값을 반환함.\n",
    "\n",
    "    ```python\n",
    "    array([-0.01258566, -0.00156614, 0.04207708, -0.00180545])\n",
    "    ```\n",
    "    \n",
    "    * 0번 인덱스: 카트의 위치 (0.0은 중앙)\n",
    "    * 1번 인덱스: 카트의 이동 속도 (음수는 왼쪽 방향으로의 이동 의미)\n",
    "    * 2번 인덱스: 막대(pole)의 기울어진 각도(0.0은 수직)\n",
    "    * 3번 인덱스: 막대의 각속도(양수는 시계방향 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* `gym`이 제공하는 전체 시뮬레이션 리스트는 아래와 같이 확인:\n",
    "\n",
    "    ```python\n",
    "    gym.envs.registry.all()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 환경 렌더링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경을 화면에 출력하려면 `render()` 메서드 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `render()` 메서드에서 반환된 이미지를 넘파이 배열로 받으려면 `mode=\"rgb_array\"` 설정\n",
    "\n",
    "    ```python\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `img`에는 아래 모양의 컬러 사진이 저장됨.\n",
    "\n",
    "    ```python\n",
    "    (800, 1200, 3) # img.shape\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가능한 행동은 다음과 같이 확인\n",
    "\n",
    "    ```python\n",
    "    env.action_space\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole-v1의 경우 다음과 0과 1 두 종류의 행동이 가능:\n",
    "\n",
    "    ```python\n",
    "    Discrete(2)\n",
    "    ```\n",
    "    \n",
    "    * 0: 왼쪽으로 가속하기\n",
    "    * 1: 오른쪽으로 가속하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경에 따라 다른 행동, 심지어 연속적인 값, 즉, 부동소수점으로 표현되는 행동도 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: 초기 상태에서 막대가 오른쪽으로 쓰러지고 있기에 오른쪽으로 가속하는 행동 한 번 실행\n",
    "\n",
    "    ```python\n",
    "    action = 1  # 오른쪽으로 (살짝) 가속\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동 결과는 아래와 같음:\n",
    "\n",
    "    ```python\n",
    "    # obs: 한 번 오른쪽으로 가속한 후 관측값\n",
    "    array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])\n",
    "\n",
    "    # reward: CartPole의 경우 보상은 항상 1\n",
    "    1.0\n",
    "\n",
    "    # done: 게임 종료 여부, 즉, 막대가 쓰러졌는지 여부\n",
    "    False\n",
    "\n",
    "    # info: 에이전트 관련 기타 정보. CartPole의 경우 없음.\n",
    "    {}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책 활용 예제: 하드 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책: 막대가 왼쪽으로 쓰러지면 왼쪽으로 가속, 오른쪽으로 쓰러지면 오른쪽으로 가속\n",
    "\n",
    "    ```python\n",
    "    def basic_policy(obs):\n",
    "        angle = obs[2]\n",
    "        return 0 if angle < 0 else 1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 위 정책을 이용하여 에피소드 500번 시뮬레이션 실행. \n",
    "    * 매 에피소드마다 얻는 보상 저장\n",
    "    * 여기서 누적되는 보상은 게임이 종료될 때까지의 행동 실행횟수를 가리킴.\n",
    "\n",
    "    ```python\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(200):\n",
    "            action = basic_policy(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 평균적으로 41.7회, 최대 68회 행동 실행함.\n",
    "    \n",
    "    ```python\n",
    "    # np.mean(totals), np.std(totals), np.min(totals), np.max(totals) \n",
    "    (41.718, 8.858356280936096, 24.0, 68.0)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 막대가 오래 서있지 못하고 좌우로 심하게 흔들리면서 쓰러짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망을 활용한 정책 알고리즘을 이용하면 더 좋은 정책을 생성함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4절 신경망 정책"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "핸즈온머신러닝_7장.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
