{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 18장 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 감사의 글\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 머신러닝 분야에서 가장 흥미로우며 가장 오래된 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 게임, 기계 제어 등 매우 다양한 애플리케이션에서 활용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 강화학습(Reinforcement Learning) 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 심층 강화학습의 주요 기법 두 가지\n",
    "    * 정책 그레이디언트(policy gradients)\n",
    "    * 심층 Q-네트워크\n",
    "        * 마르코프 결정과정(Markov decision processes, MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 실전 예제 1: 움직이는 카드(cart)에서 막대 균형잡기\n",
    "    * OpenAI-Gym 소개\n",
    "    * 정책 그레이디언트 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실전 예제 2: 브레이크아웃(Bradkout)이라는 아타리(Atari) 게임 플레이어 훈련시키기\n",
    "    * TF-Agents 라이브러리 소개\n",
    "    * 심층 Q-네트워크 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1절 보상 최적화 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 소프트웨어 에이전트(agent)가 주어진 환경에서 관측(observation) 후 행동(action)을 취하는 행위 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행위 결과에 따라 양(positive) 또는 음(negative)의 보상을 받음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목표: 최대한의 (양의) 보상과 최소한의 (음의) 보상 받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 에이전트 학습 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 주요 용어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트\n",
    "\n",
    "* 환경\n",
    "\n",
    "* 관측\n",
    "\n",
    "* 행동\n",
    "\n",
    "* 보상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-01.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 1: 로봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 로봇 제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 실제 세상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 카메라, 터치 센서 등을 이용하여 환경 관찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 모터를 구동하기 위해 시그널 전송"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 목적지에 도착할 때 양의 보상, 시간을 낭비하거나 잘못된 방향으로 향할 때 음의 보상 받음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 2: 미스 팩맨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 미스 팩맨 제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 아타리 게임 시뮬레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 스크린샷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 가능한 아홉 가지 조이스틱 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 게임 점수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 3: 바둑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 미스 팩맨과 유사하게 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 4: 온도조절기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 온도제어 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 주위 온도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 온도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 온도 조절\n",
    "    * 사람의 요구를 예측하도록 학습된 결과에 따라 행동 취함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 에너지를 절약하면 양의 보상, 사람이 온도를 조작할 필요가 발생하면 음의 보상을 받음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활용 사례 5: 주식 자동매매 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트: 자동매매 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경: 주식시장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측: 주식시장 가격"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동: 매초 얼마나 사고팔아야 할지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상: 금전적 이익과 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 주요 활용 영역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 자율주행 자동차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 추천 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹페이지 상에 광고배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지 분류시스템 포커싱(주의집중) 영역 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 보상의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 양의 보상: 기쁨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 음의 보상: 아픔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 양 또는 음의 보상이 전혀 없을 수도 있음\n",
    "    * 미로 게임 에이전트는 타임스텝마다 음의 보상을 받기에 빠르게 탈출하도록 학습함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2절 정책 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 에이전트(agent)가 행동을 결정하기 위해 사용하는 알고리즘\n",
    "    * 입력값: 관측\n",
    "    * 출력: 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 입력값이 필요없을 수도 있음\n",
    "    * 예제: 30분 동안 수집한 먼지 양을 보상으로 받는 로봇진공청소기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-02.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 무작위성이 포함된 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제: 로봇진공청소기\n",
    "    * 매 초마다 지정된 확률 p 만큼 전진.\n",
    "    * 무작위적으로 (1-p) 의 확률로 왼쪽 또는 오른쪽으로 회전하기.\n",
    "    * 회전 각도는 -r 과 r 사이의 임의의 값.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책 파라미터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 파라미터**: 정책에 사용되는 변경가능한 파라미터\n",
    "    * 로봇진공청소기의 경우: p 와 r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 탐색**: 가장 성는이 좋은 정책 마라미터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **정책 공간**: 정책 파라미터가 취할 수 있는 값들의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 정책 탐색 기법: 유전 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제\n",
    "    * 1세대: 정책 100개 랜덤하게 생성하여 사용해본 후 상위 20% 정책만 남김\n",
    "    * 2세대: 남겨진 20개을 정책을 각각 4개씩 약간의 무작위성을 추가하여 복사.\n",
    "    * 위 과정을 좋은 정책을 찾을 때까지 여러 세대에 걸쳐 반복.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주요 활용 예제: NEAT(NeuroEvolution of Augmenting Topologies) 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 정책 탐색 기법: 정책 그레이디언트(PG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경사하강법과 유사한 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책을 따른 결과인 보상이 최댓값을 갖도록 정책 파라미터들을 조금씩 변경하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보상이 최댓값을 갖도록 하기에 **경사상승법** 이라고도 불림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: 로봇 진공청소기\n",
    "    * p의 값을 조금 크게 한 경과 30분 동안 더 많은 먼지를 수집할 경우 p의 값을 좀 더 키움.\n",
    "    그렇지 않으면 p의 값을 조금 줄임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PG에 대해 이번 장에서 자세히 다룰 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PG를 TF(텐서플로우)로 구현하려면 에이전트가 활동할 환경을 먼저 세팅해야 함.\n",
    "    여기서는 **Open AI Gym** 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3절 Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 아타리 게임, 보드게임, 2D/3D 물리적 시물레이션 등을 위한 시뮬레이션 환경 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시뮬레이션 환경 내에서 에이전트에 대한 다양한 정책을 훈련하고 비교할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 설치 요령은 책과 코랩 노트북 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CartPole 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 카드 위에 놓인 막대가 넘어지지 않도록 오른쪽/왼쪽으로 가속할 수 있는 2D 시뮬레이션 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-03.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CartPole 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 환경 리셋하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole-v1 환경을 생성한 후 리셋하여 환경 초기화 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset() # 초기화된 환경 관측치 할당\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* CartPole-v1의 경우 `reset()` 메서드는 아래 모양의 초기화된 환경 관측값을 반환함.\n",
    "\n",
    "    ```python\n",
    "    array([-0.01258566, -0.00156614, 0.04207708, -0.00180545])\n",
    "    ```\n",
    "    \n",
    "    * 0번 인덱스: 카트의 위치 (0.0은 중앙)\n",
    "    * 1번 인덱스: 카트의 이동 속도 (음수는 왼쪽 방향으로의 이동 의미)\n",
    "    * 2번 인덱스: 막대(pole)의 기울어진 각도(0.0은 수직)\n",
    "    * 3번 인덱스: 막대의 각속도(양수는 시계방향 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* `gym`이 제공하는 전체 시뮬레이션 리스트는 아래와 같이 확인:\n",
    "\n",
    "    ```python\n",
    "    gym.envs.registry.all()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 환경 렌더링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경을 화면에 출력하려면 `render()` 메서드 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `render()` 메서드에서 반환된 이미지를 넘파이 배열로 받으려면 `mode=\"rgb_array\"` 설정\n",
    "\n",
    "    ```python\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `img`에는 아래 모양의 컬러 사진이 저장됨.\n",
    "\n",
    "    ```python\n",
    "    (800, 1200, 3) # img.shape\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가능한 행동은 다음과 같이 확인\n",
    "\n",
    "    ```python\n",
    "    env.action_space\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole-v1의 경우 다음과 0과 1 두 종류의 행동이 가능:\n",
    "\n",
    "    ```python\n",
    "    Discrete(2)\n",
    "    ```\n",
    "    \n",
    "    * 0: 왼쪽으로 가속하기\n",
    "    * 1: 오른쪽으로 가속하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 환경에 따라 다른 행동, 심지어 연속적인 값, 즉, 부동소수점으로 표현되는 행동도 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: 초기 상태에서 막대가 오른쪽으로 쓰러지고 있기에 오른쪽으로 가속하는 행동 한 번 실행\n",
    "\n",
    "    ```python\n",
    "    action = 1  # 오른쪽으로 (살짝) 가속\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 행동 결과는 아래와 같음:\n",
    "\n",
    "    ```python\n",
    "    # obs: 한 번 오른쪽으로 가속한 후 관측값\n",
    "    array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])\n",
    "\n",
    "    # reward: CartPole의 경우 보상은 항상 1\n",
    "    1.0\n",
    "\n",
    "    # done: 게임 종료 여부, 즉, 막대가 쓰러졌는지 여부\n",
    "    False\n",
    "\n",
    "    # info: 에이전트 관련 기타 정보. CartPole의 경우 없음.\n",
    "    {}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정책 활용 예제: 하드 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책: 막대가 왼쪽으로 쓰러지면 왼쪽으로 가속, 오른쪽으로 쓰러지면 오른쪽으로 가속\n",
    "\n",
    "    ```python\n",
    "    def basic_policy(obs):\n",
    "        angle = obs[2]\n",
    "        return 0 if angle < 0 else 1\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 위 정책을 이용하여 에피소드 500번 시뮬레이션 실행. \n",
    "    * 매 에피소드마다 얻는 보상 저장\n",
    "    * 여기서 누적되는 보상은 게임이 종료될 때까지의 행동 실행횟수를 가리킴.\n",
    "\n",
    "    ```python\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(200):\n",
    "            action = basic_policy(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 평균적으로 41.7회, 최대 68회 행동 실행함.\n",
    "    \n",
    "    ```python\n",
    "    # np.mean(totals), np.std(totals), np.min(totals), np.max(totals) \n",
    "    (41.718, 8.858356280936096, 24.0, 68.0)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 막대가 오래 서있지 못하고 좌우로 심하게 흔들리면서 쓰러짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망을 활용한 정책 알고리즘을 이용하면 더 좋은 정책을 생성함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4절 신경망 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 관측을 입력받아 실행할 행동을 결정하는 데에 사용되는 값을 반환하는 신경망 정책함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망 정책함수의 반환값: 실행할 행동에 대한 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실제 행동은 행동에 대한 확률에 의거하여 무작위적으로 선택\n",
    "    * 이유: 새로운 활동에 대한 가능성을 열어 두었을 때 보다 나은 정책을 찾을 수도 있기 때문임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-04a.png\" width=\"300\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동 최종 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 확률적으로 행동을 결정하는 것 이외에 과거의 행동과 관측을 행동을 결정하는 데에 활용할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole 문제의 경우에는 해당되지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하지만 만약에 카트의 현재 위치만 관측된다면 현재 속도를 추정하기 위해 이전 관측도 활용해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.keras 를 활용한 신경망 정책 모델\n",
    "    * 출력층: 한 개의 뉴런과 시그모이드 활성화 함수 사용\n",
    "        * 좌우 움직임 이외의 다른 행동이 가능한 경우 그만큼의 뉴런 사용하며\n",
    "            최종적으로 소프트맥스 활성화 함수 활용\n",
    "    * 출력값: 0과 1사이의 확률값 반환.\n",
    "        여기서는 왼쪽으로 행동할 확률 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "n_inputs = 4 # 관측값 모양(env.observation_space.shape[0])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5절 행동 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 지도학습 신경망 모델의 일반적인 학습법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수(`loss`)를 기준으로 경사하강법(`optimizer`) 적용\n",
    "* 평가지표\n",
    "    * 회귀모델: (일반적으로) 손실 기준\n",
    "    * 분류모델: 정확도(accuracy), ACU 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 회귀 모델\n",
    "    ```python\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=\"adam\")\n",
    "    model.fit()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 분류 모델\n",
    "    ```python\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=\"nadam\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    model.fit()\n",
    "    ```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 강화학습 신경망 모델 평가지표: 행동 이익"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 행동결정 단계에서의 가장 좋은 행동이 무엇인지 알고 있다면 일반적인 지도학습 활용 가능\n",
    "    * `loss`를 예를 들어 추정된 확률과 타깃 확률 사이의 크로스 엔트로피로 정할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하지만 강화학습에서는 에이전트가 활용할 수 있는 것은 보상뿐임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 따라서 특정 행동의 영향력을 평가할 수 있는 기준이 요구됨.\n",
    "    * 행동 평가 = 행동에 따른 보상 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 신용할당 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특정 행동이 보상에 미치는 영향을 평가하기가 매우 어려움을 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동의 결과가 늦게 보상을 줄 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 또한 어떤 행동이 보상에 얼마나 영향을 미쳤는가를 파악하기 어려움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동 대가(action return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신용할당 문제의 해결 전략: \n",
    "    각 행동 결정단계마다 **할인계수**(discount factor) **감마**($\\gamma$) 를 \n",
    "    적용한 보상을 모두 합하여 행동 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **행동 대가**(action return): 할인된 미래 보상의 누적 합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제\n",
    "    * 아래 그림에서 에이전트가 오른쪽으로 세 번 움직이는 행동을 취함.\n",
    "        * 첫째 스텝에서 10, 둘째 스텝에서 0, 셋째 스텝에서 -50의 보상을 받음.\n",
    "    * 따라서 첫째 행동의 대가는 다음과 같이 계산됨:\n",
    "    \n",
    "        $$\n",
    "        10 + \\gamma \\cdot 0 + \\gamma^2 \\cdot (-50) = -22\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-05.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 할인계수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0과 1 사이의 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1에 가까울 수록 보다 먼 미래의 보상이 보다 중요해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole의 경우\n",
    "    * 행동의 효과가 매우 짧은 기간안에 나타남.\n",
    "    * 따라서 $\\gamma = 0.95$ 가 적절해 보임. ($0.95^{13}\\approx 0.5$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동이익(action advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 대가 정규화: 대가들의 평균과 표준편차를 계산한 후 표준점수로 변환하기\n",
    "\n",
    "    $$\n",
    "    Z = \\frac{X - \\mu(X)}{\\sigma(X)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동 이익: 게임을 충분히 많이 실행한 후 계산한 각 행동에 대한 대가를 정규화한 값\n",
    "    * 많은 에피소드(시물레이션)를 실행하면서 얻은 각 행동에 대한 대가들의 평균값과 표준편차를 계산함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 행동 평가지표: 행동이익"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 양의 행동이익: 좋은 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 음의 행동이익: 나쁜 행동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6절 정책 그레이디언트(Policy Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* PG: 높은 보상을 얻도록 신경망 정책 모델의 파라미터를 경사하강법으로 학습시키는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### REINFORCE 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 대표적인 PG 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 작동방식 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 신경망 정책 모델을 활용한 게임을 여러 번의 에피소드로 실행하면서 아래 데이터 저장\n",
    "    1. 시뮬레이션의 매 스텝마다 파라미터별로 그레이디언트 계산해서 저장.\n",
    "    1. 시뮬레이션의 매 스텝마다 보상을 확인해서 저장\n",
    "    <br><br>\n",
    "1. 모든 에피소드 완료 후 매 스텝의 행동이익 계산\n",
    "    <br><br>\n",
    "1. 매 스텝에 대해 행동이익을 저장된 파라미터 그레이디언트와 곱하기\n",
    "    <br><br>\n",
    "1. 계산된 모든 그레이디언트의 평균값을 파라미터별로 계산\n",
    "    <br><br>\n",
    "1. 계산된 파라미터별 그레이디언트의 평균값을 신경망 정책 모델에 경사하강법을 이용하여 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 구현: 스텝 실행 결과 반환 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스텝을 한 번 실행할 때마다 관측과 그레이디언트 반환하기 함수\n",
    "    <br><br>\n",
    "* 타깃확률(`y_target`): 0 또는 1\n",
    "    * 정책 모델이 추천하는 방향을 타깃확률로 지정\n",
    "    * 따라서 손실(`loss`)이 최소, 즉, 정책 모델이 가능한 좋은 확률로 행동을 추천하는 방향으로 유도함.\n",
    "    <br><br>\n",
    "* 그레이디언트(`grads`): 실행된 스텝에서 각 파라미터에 대한 손실함수의 그레이디언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 구현: 여러 에피소드 실행 결과 반환 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에피소드 실행중에 발생하는 매 스텝의 결과인 보상과 그레이디언트를 리스트로 저장\n",
    "    <br><br>\n",
    "* 모든 에피소드에 대해 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 구현: 행동이익 반환 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `discount_rewards()` 함수\n",
    "    * 하나의 에피소드 내에서 발생하는 스텝별 보상에 대한 행동대가(return) 계산\n",
    "    <br><br>\n",
    "* `discount_and_normalize_rewards()` 함수\n",
    "    * 모든 에피소드에 대해 스텝별 행동이익 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 구현: 신경망 정책 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `discount_rewards()` 함수\n",
    "    * 하나의 에피소드 내에서 발생하는 스텝별 보상에 대한 행동대가(return) 계산\n",
    "    <br><br>\n",
    "* `discount_and_normalize_rewards()` 함수\n",
    "    * 모든 에피소드에 대해 스텝별 행동이익 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "n_iterations = 150          # PG 적용을 150번 실행\n",
    "n_episodes_per_update = 10  # 10번 에피소드마다 PG 적용\n",
    "n_max_steps = 200           # 매 에피소드마다 최대 200번 행동\n",
    "discount_rate = 0.95        # 할인계수\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy  # 손실함수\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# PG 적용: 150회\n",
    "for iteration in range(n_iterations):\n",
    "    # 에피소드 10번 실행\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    \n",
    "    # 스텝별 그레이디언트에 스텝별 행동이익을 곱한 후 파라미터별 그레이디언트 평균값 계산\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "        \n",
    "    # 계산된 파라미터별 그레이디언트를 기존의 파라미터에 더하는 방식으로 경사하강법 적용\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평균 보상이 200에 거의 가까울 정도로 학습이 잘됨(구글 코랩 노트북 참조)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하지만 학습을 위해 매우 많은 게임을 실행해야 함. 즉, 샘플 효율성이 매우 낮음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CartPole 용도의 정책 그레이디언트 알고리즘을 다른 문제에 활용하기에는 알고리즘이 너무 단순함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하지만 나중에 짧게 소개할 Actor-Critic 알고리즘의 기초로 사용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7절 마르코프 결정과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* PG 알고리즘: 보상을 증가시키기 위해 정책을 직접 최적화하는 방향으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다른 알고리즘: PG 보다 덜 직접적으로 학습\n",
    "    * 에이전트가 새로운 스텝을 실행하기 전의 (환경)상태에서 기대할 수 있는 대가를 추정하거나,\n",
    "        취할 수 있는 각각의 행동에 대한 대가를 추정함.\n",
    "    * 예제:\n",
    "        * 가치 반복 알고리즘\n",
    "        * 시간차 학습\n",
    "        * Q-러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 마르코프 결정과정(Markov Decision Process, MDP)\n",
    "    * 가치 반복 알고리즘, 시간차 학습, Q-러닝 등에 사용되는 행동 결정과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 마르코프 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 20세기 초에 마르코프의 메모리 없는 확률과정 연구에 사용된 개념\n",
    "    * 확률과정(stochastic process): \n",
    "        * 확률공간에서 정의되는 확률변수들의 모임\n",
    "        * 확률변수의 인덱스는 정수를 취하여 이산적일 수도 있고, 실수를 취하여 연속적일 수도 있음.\n",
    "        * 확률변수 사이의 이동은 확률적으로 이루어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제\n",
    "    * 확률변수: 상태(state)\n",
    "        * $\\mathsf{S}_0, \\mathsf{S}_1, \\mathsf{S}_2, \\mathsf{S}_3$\n",
    "    * 상태 사이의 이동확률: 두 개의 상태에만 의존함. (메모리 없음)\n",
    "        * $\\mathsf{S}_0$ 상태인 경우\n",
    "            * 70%의 확률로 자신의 상태에 머무름\n",
    "            * 20%의 확률로 $\\mathsf{S}_1$ 상태로 이동\n",
    "            * 10%의 확률로 $\\mathsf{S}_3$ 상태로 이동\n",
    "        * 기타 등등(아래 그림 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-06.png\" width=\"300\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 마르코프 결정과정(MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1950년대에 Bellman에 의해 소개됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 마르코프 체인과 유사하지만, 각 상태에서 에이전트가 다양한 행동 중에 하나의 행동을 취할 수 있음.\n",
    "    * 다른 상태로의 이동은 취한 행동에 의존함.\n",
    "    * 다른 상태로 이동하면서 경우레 따라 보상을 받기도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 에이전트의 목표: 최대의 보상을 받는 정책 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### MDP 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-07.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 각 상태에서 시간이 흐르면서 최고의 보상을 받을 수 있는 전략은?\n",
    "    * $\\mathsf{S}_0$: 행동 $a_0$ 선택하기\n",
    "    * $\\mathsf{S}_1$: 보상이 전혀 없는 행동 $a_0$ 또는 \n",
    "        위험하지만 궁극적으로 높은 보상의 가능성을 갖는 행동 $a_2$ 선택 가능\n",
    "    * $\\mathsf{S}_2$: 선택의 여지 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 최적의 상태 가치(optimal state value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책 평가 용도로 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $V^*(s)$: 상태 $s$ 에서 에이전트가 최적의 행동을 선택한다고 가정했을 때\n",
    "    얻을 수 있는 할인된 미래 보상에 대한 기대치의 최댓값\n",
    "    \n",
    "    $$\n",
    "    V^*(s) = \\max_a \\sum_{s'} T(s, a, s')\\cdot \\big [R(s, a, s') + \\gamma \\cdot V(s') \\big]\n",
    "    $$\n",
    "    \n",
    "    * $T(s, a, s')$: 행동 $a$를 선택했을 때 상태 $s$에서 상태 $s'$로 전이될 확률\n",
    "    * $R(s, a, s')$: 행동 $a$를 선택했을 때 상태 $s$에서 상태 $s'$로 이동되었을 때 받을 수 있는 보상\n",
    "    * $\\gamma$: 할인계수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 동적계획법 활용: $V^*(s)$를 동적계획법으로 빠르게 계산 가능\n",
    "\n",
    "    $$\n",
    "    V_{k+1}^*(s) \\leftarrow \\max_a \\sum_{s'} T(s, a, s')\\cdot \\big [R(s, a, s') + \\gamma \\cdot V_k(s') \\big]\n",
    "    $$\n",
    "    \n",
    "    * $V_k^*(s)$: 동적계획법 알고리즘의 $k$번째 반복에서 상태 $s$의 추정 상태 가치\n",
    "    * $V_0^*(s) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-가치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최적의 상태-행동(state-action) 가치 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $Q^*(s, a)$: 에이전트가 상태 $s$에 도착한 후에 행동 $a$를 선택할 때 얻을 수 있는 \n",
    "    할인된 미래 보상에 대한 기대치\n",
    "    * 에이전트를 위한 최적의 정책을 결정하는 데에 활용될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 동적계획법 활용: $Q^*(a, s)$를 동적계획법으로 빠르게 계산 가능\n",
    "\n",
    "    $$\n",
    "    Q_{k+1}^*(a, s) \\leftarrow \\sum_{s'} T(s, a, s')\\cdot \\big [R(s, a, s') + \n",
    "                            \\gamma \\cdot \\max_{a'}Q_k(s', a') \\big]\n",
    "    $$\n",
    "    \n",
    "    * $Q_0^*(a, s) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\pi^*(s)$: 상태 $s$에 도착했을 때 취할 수 있는 최선의 정책은 최고의 Q-가치를 갖는 행동 선택하기\n",
    "\n",
    "    $$\n",
    "    \\textrm{argmax}_a\\, Q^*(s, a)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 적용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\pi^*()$ 함수를 위 그림에 있는 MDP에 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* MDP 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "transition_probabilities = [ # 모양=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # 모양=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Q-가치 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Q_values = np.full((3, 3), -np.inf) # 불가능한 행동: -np.inf\n",
    "\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # 가능한 행동: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Q-가치 반복\n",
    "    * 할인계수: `gamma` = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 결과: Q-가치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "array([[18.91891892, 17.02702702, 13.62162162],\n",
    "       [ 0. , -inf, -4.87971488], \n",
    "       [ -inf, 50.13365013, -inf]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* `gamma=0.90`인 경우: `np.argmax(Q_values, axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "array([0, 0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `gamma=0.95`인 경우: `np.argmax(Q_values, axis=1)`\n",
    "    * 에이전트가 미래에 대한 보상을 보다 높게 간주함. \n",
    "    * 상태 $s_1$에서 당장의 고통(불길, -50)을 감수하고 행동 $a_2$ 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "array([0, 2, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8절 시간차 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 학습 초기에 에이전트는 MDP에 대한 사전정보를 최소한만 알고 있음.\n",
    "    * 가능한 상태와 가능한 행동은 안다고 가정.\n",
    "    * 반면에 행동에 대한 보상과 전이확률은 모름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시간차 학습(Time Difference Learning, TD 학습)을 통해 보상과 전이확률 추정\n",
    "    * 보상: 한 번 이상 각각의 상태와 전이를 경험해서 확인\n",
    "    * 전이 확률: 여러 번의 경험을 통해 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TD 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 탐험 정책: 완전히 랜덤한 정책 등을 이용하여 MDP를 탐험하는 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 탐험이 진행하면서 실제로 관측된 전이와 보상에 근거하여 상태 가치 추정값 업데이트\n",
    "\n",
    "    $$\n",
    "    V_{k+1}(s) \\leftarrow (1-\\alpha) V_k(s) + \\alpha \\big( r + \\gamma\\cdot V_k(s') \\big)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 아래와 같이 표현 가능:\n",
    "\n",
    "    $$\n",
    "    V_{k+1}(s) \\leftarrow V_k(s) + \\alpha \\cdot \\delta_k(s, r, s')\n",
    "    $$\n",
    "    \n",
    "    단, \n",
    "    \n",
    "    $$\n",
    "    \\delta_k(s, r, s') = r + \\gamma\\cdot V_k(s') - V_k(s)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha$: 학습률 (0.01 정도로 작게)\n",
    "* $r + \\gamma\\cdot V_k(s')$: TD 타깃\n",
    "* $\\delta_k(s, r, s')$: TD 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 아래 식을\n",
    "\n",
    "    $$\n",
    "    V_{k+1}(s) \\leftarrow (1-\\alpha) V_k(s) + \\alpha \\big( r + \\gamma\\cdot V_k(s') \\big)\n",
    "    $$\n",
    "\n",
    "    다음과 같이 표현하는 것 선호됨\n",
    "\n",
    "    $$\n",
    "    V(s) \\,\\underset{\\alpha}{\\leftarrow}\\, r + \\gamma\\cdot V(s')\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 9절 Q-러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* TD 학습 방식을 Q-가치를 추청하는 데에 사용함.\n",
    "\n",
    "    $$\n",
    "    Q(s, a) \\,\\underset{\\alpha}{\\leftarrow}\\, r + \\gamma\\cdot \\max_{a'} Q_(s', a')\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TD 학습을 통해 알아낸 보상과 전이확률을 이용하여 Q-러닝을 반복실행하면 최적의 Q-가치에 수렴함.\n",
    "    대신, 보다 훨씬 많은 반복이 요구됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-08.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### off-policy 대 on-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* off-policy 알고리즘: 학습 과정중에 사용되는 정책이 반드시 최종 적으로 실행되는 정책이 아닐 수도 있는 알고리즘\n",
    "    * 예제: Q-러닝 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* on-policy 알고리즘: 학습 과정에 사용되는 정책이 항상 사용되는 알고리즘\n",
    "    * 예제: PG 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 탐험 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\varepsilon$-탐욕 정책\n",
    "    * 각 스텝에서 $\\varepsilon$ 확률로 랜덤하게 행동을 선택하거나 $(1-\\varepsilon)$의 확률로\n",
    "    그 순간 가장 최선인 행동을 선택하는 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 탐험함수 적용 정책\n",
    "    * 이전에 많이 시도하지 않았던 행동을 시도하도록 유도하는 정책\n",
    "    \n",
    "    $$\n",
    "    Q(s, a) \\,\\underset{\\alpha}{\\leftarrow}\\, r + \\gamma\\cdot \\max_{a'} f(Q_(s', a'), N(s',a'))\n",
    "    $$   \n",
    "    \n",
    "    * $N(s',a')$: 상태 $s'$에서 행동 $a'$을 선택한 횟수\n",
    "    * $f(Q, N)$은 아래와 같은 탐험 함수\n",
    "    \n",
    "        $$f(Q, N) = Q + \\frac{\\kappa}{1+N}$$\n",
    "        \n",
    "        * $\\kappa$: 탐험 호기심 정도를 나타내는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 근사 Q-러닝과 심층 Q-러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 근사 Q-러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-러닝의 문제점: 중간규모 이상의 MDP에 적용하기 어려움.\n",
    "    이유: 너무 많은 상태의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임의의 상태-행동 $(s, a)$에 대한 근사 Q-가치 $Q_\\theta(s,a)$를 대신 계산하여 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 심층 Q-러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2013년 딥마인드가 제시한 심층신경망을 활용한 $Q_\\theta(s,a)$ 추정 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 심층 Q-네트워크(DQN, Deep Q-Network): Q-가치를 추정하기 위해 사용하는 DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 심층 Q-러닝: 근사 Q-러닝을 위해 DQN을 활용하는 학습법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DQN 훈련 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행동을 결정해야 하는 매 순간(상태)에 이전 경험을 바탕으로 정해진 타깃 Q-가치를 목표로 지도학습 실행\n",
    "    * 타깃 Q-가치는 정해진 배치(batch) 크기 만큼 무작위적으로 선택된 이전 경험으로 결정.\n",
    "    \n",
    "    $$\n",
    "    Q_{\\textit{target}}(s, a) = r + \\gamma\\cdot \\max_{a'} Q_\\theta(s', a')\n",
    "    $$       \n",
    "    \n",
    "    * 다수의 에피소드를 통한 업데이트 반복 알고리즘으로 이해하려면 아래 식이 보다 적절함.\n",
    "        $Q_{\\textit{target}}(s, a)$은 매 에피소드마다 업데이트됨.\n",
    "\n",
    "    $$\n",
    "    Q_{\\textit{target}}(s, a) \\leftarrow r + \\gamma\\cdot \\max_{a'} Q_\\theta(s', a')\n",
    "    $$           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10절 심층 Q-러닝 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 구현: DQN 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = [4] # 관측 자료형 모양\n",
    "n_outputs = 2 # 행동 종류 2개\n",
    "\n",
    "# 출력층 뉴련 수: 2개. \n",
    "# 즉, 현재 상태에서 취할 수 있는 모든 행동에 대한 확률값 반환\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 구현: $\\varepsilon$-탐욕 정책 알고리즘\n",
    "\n",
    "* DQN 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 구현: 지정된 크기의 경험 선택 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 구현: 스텝 실행 알고리즘\n",
    "\n",
    "* DQN 모델을 활용하는 $\\varepsilon$-탐욕 정책을 활용하여 한 스템 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 구현: 지정된 batch 크기의 경험을 이용하여 설정된 타깃 Q-가치를 활용한 경사하강법 실행 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CartPole의 DQN 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for episode in range(600):\n",
    "    obs = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* DQN 알고리즘의 학습곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-09a.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 240 번의 에피소드 동안 발전이 전혀 없다가 갑자기 좋아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이전 Q-러닝 알고리즘보다 훨씬 빠르기 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단, 에피소드가 더 지마면 망각현상이 발생하여 성능이 오르락내리락 함.\n",
    "    이런 현상을 **재해성 망각**(catastrophic forgetting)이라 부름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DQN 모델의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련이 매우 어렵고 불안정한 경우가 일반적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 초기 하이퍼파라미터 값과 랜덤 시드에 영향을 많이 받음.즉, 운이 매우 좋아야 함.\n",
    "    * 예제: CartPole의 경우 은닉층의 뉴런 수를 30 또는 34로 정하면 성능이 100 이상 나오지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그럼에도 불구하고 알파고와 아타리 게임 등 몇몇 실전 앱에서 훌륭하게 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 11절 심층 Q-러닝의 변종"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞서 설명한 CartPole의 DQN 모델은 너무 불안정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 안정적이면서 빠른 훈련을 지원하는 심층 Q-러닝 알고리즘 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 고정 Q-가치 타깃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞서 설명한 모델은 하나의 모델이 타깃 Q-가치와 현재 상태에서의 예측을 함께 실행함.\n",
    "    따라서 자기 꼬리를 물려고 하는 강아지처럼 불안정한 피드백의 요인으로 작용하여 발산, 진동, 동경 등의 문제 발생 유발."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2013년 딥마인드 팀에서 아타리 게임 구현에 활용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 타깃 Q-가치를 정하는 모델을 별도로 사용\n",
    "    * 온라인 모델: 각 스텝에서 학습하고 에이전트를 움직임에 사용되는 행동을 선택하는 모델\n",
    "    * 타깃 모델: 타깃을 정의하기 위해서만 사용되는 모델. 온라인 모델의 복사본 사용.\n",
    "\n",
    "    ```python\n",
    "    next_Q_values = target.predict(next_states)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일정 숫자의 에피소드를 진행할 때마다 경사하강법을 적용하여 파라미터 조정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2013년 딥마인드 모델 (아타리 게임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습률: 0.00025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 타깃 모델 업데이트 주기: 10,000 에피소드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경험 저장 버퍼 크기: 100만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\epsilon$: 100만 스텝동안 1에서 0.1까지 매우 천천히 감소시킴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 매 에피소드에서의 스텝 수: 5천만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 더블 DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2015년 딥마이드가 2013년 모델을 개선해서 제시함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2013년도 모델에서 사용된 타깃 모델의 타깃 Q-가치 계산 방법을 조금 수정함.\n",
    "    * 온라인 모델이 선정한 최적의 행동을 기준으로 타깃 모델에서 타깃 Q-가치 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    [...] # 이전과 동일\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 보다 안정적으로 학습됨을 확인 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch18/homl18-10.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 우선순위 기반 경험 재생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2015년 딥마인드에서 제시한 개선 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 타깃 Q-가치 계산에 사용되는 이전 경험을 무작위적으로 선택하는 것 대신에 \n",
    "    중요한 경험을 보다 자주 선택하도록 유도하는 기법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 중요도 평가 기준: TD-오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 모델에 따라 중요도를 어떻게 활용할지 달라짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 듀얼링 DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보통 DDQN 이라 부름.\n",
    "    * 주의: 더블 DQN과 혼동하지 말 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2015년 딥마인드에서 제시한 개선 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-가치가 아래처럼 계산될 수 있음에 주목함.\n",
    "\n",
    "    $$\n",
    "    Q(s,a) = V(s) + A(s, a)\n",
    "    $$\n",
    "    \n",
    "    * $V(s)$: 상태 $s$의 가치\n",
    "    * $A(s, a)$: 상태 $s$에서 다른 가능한 모든 행동과 비교하여 행동 $a$를 취했을 때 얻는 이득(advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래 식을 만족시키는 행동 $a^*$ 존재\n",
    "    \n",
    "    $$V(s) = Q(s,a^*) \\quad\\text{이고}\\quad A(s, a^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DDQN 모델: 상태의 가치($V(s)$)와 모든 가능한 행동의 이득($A(s,a)$)을 계산하여 Q-가치 추정치 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "K = keras.backend\n",
    "input_states = keras.layers.Input(shape=[4])\n",
    "\n",
    "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "\n",
    "state_values = keras.layers.Dense(1)(hidden2)\n",
    "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
    "\n",
    "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "\n",
    "Q_values = state_values + advantages\n",
    "\n",
    "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 강화학습 모델 활용법 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여러 모델을 조합하여 새로운 모델 생성하여 많이 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥마인드(2017년)\n",
    "    * 6개의 기법을 조합하여 레인보우(Rainbow)라는 에이전트에 적용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하지만 강화학습 모델을 훈련시키는 일은 일반적으로 매우 어려움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 따라서 TF-Agents 등 높은 확장성과 성능이 검증된 라이브러리 활용을 추천함."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "핸즈온머신러닝_7장.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
