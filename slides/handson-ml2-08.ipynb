{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 8장 차원축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 감사의 글\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 차원의 저주: 샘플의 특성이 너무 많으면 학습이 매우 어려워짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 차원축소: 특성 수를 (크게) 줄여서 학습 불가능한 문제를 학습 가능한 문제로 만드는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 차원을 축소하면 정보손실 발생하여 성능 저하 가능. 하지만 학습속도 빨라짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: MNIST\n",
    "    * 사진의 중앙에만 집중해도 숫자 인식에 별 문제 없음.\n",
    "    * 주성분분석(PCA) 기법을 이용하여 154개의 픽셀만 대상으로 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 예제: 데이터 시각화\n",
    "    * 차원 수를 2, 3차원으로 줄이면 그래프 시각화 가능\n",
    "    * 군집 같은 시각적인 패턴을 감지하여 데이터에 대한 통찰 얻을 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYbSTqz67D9H",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 차원의 저주"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYbSTqz67D9H",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 차원축소를 위한 접근법\n",
    "    * 사영\n",
    "    * 다양체학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYbSTqz67D9H",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* PCA(주성분분석)\n",
    "    * 분산 보존\n",
    "    * 주성분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYbSTqz67D9H",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 커널 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYbSTqz67D9H",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* LLE(지역 선형임베딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-Y7yrhc7cM6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.1 차원의 저주"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 3차원을 초과하는 고차원의 공간 상상 매우 어려움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-01.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 차원의 커질 수록 두 지점 사이의 거리가 매우 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 즉, 특성 수가 아주 많은 경우, 훈련 샘플 사이의 거리가 매우 커서 과대적합 위험도가 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 이유: 두 샘플 사이의 거리가 멀어서 기존 값들을 이용한 추정(예측)이 매우 불안정해지기 때문."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 해결책: 샘플 수 늘리기. 하지만 고차원의 경우 충분히 많은 샘플 수를 준비하는 일은 사실상 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsT96srL9uAE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.2 차원축소를 위한 접근법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsT96srL9uAE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 모든 훈련 샘플이 고차원 공간 안의 저차원 부분공간에 가깝게 놓여있는 경우가 일반적으로 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 예제: 아래 두 이미지 모두 3차원(3D) 이미지를 보여줌. 하지만\n",
    "    * 왼편: 2차원(2D) 평면에 가까움.\n",
    "    * 오른편: 2차원 평면이 휘어진 모양의 롤케이크. 국소적으로 2차원 평면에 가까움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-02.png\" width=\"355\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsT96srL9uAE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 대표적인 차원축소 기법\n",
    "\n",
    "* 사영(projection)\n",
    "* 다양체학습(manifold learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 사영\n",
    "\n",
    "* $n$차원 공간에 존재하는 $d$차원 부분공간을 $d$차원 공간으로 사영하기. 단, $d < n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 적절한 사영 예제\n",
    "\n",
    "* 왼쪽 3차원에 존재하는 적절한 2차원 평면으로 사영하면 적절한 2차원 상의 이미지를 얻게됨.\n",
    "* 오른쪽 2차원 이미지에 사용된 축 $z_1$과 $z_2$를 적절하게 찾는 게 중요 과제임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-02.png\" width=\"355\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-03.png\" width=\"272\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 부적절한 사영 예제\n",
    "\n",
    "* 사영이 경우에 따라 보다 복잡한 결과를 낼 수 있음.\n",
    "* 롤케이크를 $x_1$과 $x_2$ 축으로 사영하면 샘플구분이 보다 어려워짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-06.png\" width=\"320\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다양체 학습\n",
    "\n",
    "* 롤케이크의 경우 사영 보다는 롤케이크를 펼치면 보다 적절한 2차원 이미지를 얻게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-07.png\" width=\"310\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다양체 가정\n",
    "\n",
    "* 대부분의 고차원 데이터셋이 더 낮은 차원의 다양체게 가깝다는 가정\n",
    "* 저차원의 다양체 공간으로 차원축소를 진행하면 보다 간단한 다양체가 된다라는 가정과 함께 사용됨.\n",
    "    * 하지만 일반적으로 사실 아님.\n",
    "    * 다음 그림 둘째 행 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-08.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 차원축소 기법 활용 학습 알고리즘 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사영 기법 알고리즘\n",
    "\n",
    "* PCA(주성분분석)\n",
    "* 커널 PCA (비선형 사영)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 다양체학습 기법 알고리즘\n",
    "\n",
    "* LLE(지역 선형 임베딩)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 훈련 데이터에 가장 가까운 초평면(hyperplane)을 정의한 다음, 그 평면에 사영하는 기법\n",
    "* __주성분분석__으로 알려짐.\n",
    "* 분산 보존 개념과 주성분 개념이 중요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 분산 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저차원으로 사영할 때 훈련 세트의 분산이 최대한 유지되도록 축을 지정해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예제\n",
    "\n",
    "* 아래 그림에서 $c_1$ 벡터가 위치한 실선 축으로 사영하는 경우가 분산을 최대한 보존함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-09.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 축 선택\n",
    "\n",
    "* 분산을 최대한 보존하는 축을 선택해야 함.\n",
    "* 원본 데이터셋과 사영된 데이터셋 사이의 평균제곱거리 최소화\n",
    "* 정보손실이 가장 적음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 주성분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫째 주성분: 분산을 최대한 보존하는 축\n",
    "* 둘째 주성분: 첫째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축\n",
    "* 셋째 주성분: 첫째, 둘째 주성분과 수직을 이루면서 분산을 최대한 보존하는 축\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예제\n",
    "\n",
    "* $d=2$인 경우: 첫째 주성분이 정해지문 둘째 주성분은 선택의 여지가 없음.\n",
    "    * 위 그림에서 $c_2$는 자동으로 정해짐.\n",
    "* 일반적으로 $d$차원으로 사영하고자 하는 경우: $c_1, \\dots, c_d$ 개의 주성분을 선택해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 주성분 찾기\n",
    "\n",
    "* 특잇값 분해(SVD) 기법을 이용하면 쉽게 해결됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $d$ 차원으로 사영하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $d$번째 까지의 주성분을 이용하여 데이터셋을 $d$차원으로 사영하는 일은 행렬의 곱셈으로 바로 해결됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 사이킷런 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 사이킷런의 PCA 모델 제공\n",
    "    * SVD 기법 활용\n",
    "* 예제: 데이터셋의 차원을 2로 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 밝혀진 분산 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `explained_variance_ration_` 속성 변수: 각 주성분에 대한 원 데이터셋의 분산 비율 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예제\n",
    "\n",
    "* 아래 사영 그림에서 설명된 3차원 데이터셋의 경우.\n",
    "    * $z_1$ 축: 84.2%\n",
    "    * $z_2$ 축: 14.6%\n",
    "* 따라서 마지막 주성분에는 1.2% 정도만 분산에 기여하므로 매우 적은 양의 정보가 포함되어 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-02.png\" width=\"355\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-03.png\" width=\"272\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 적절한 차원 수 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 추천 적절한 차원 수: 밝혀진 분산 비율의 합이 95% 정도 되도록 하는 주성분들로 구성\n",
    "* 단, 데이터 시각화 목적: 2개 또는 3개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 차춴 수 선택 방법 예제\n",
    "\n",
    "* 밝혀진 분산 비율의 합과 차원 수 사이의 그래프 활용\n",
    "* 밝혀진 분산의 비율의 합의 증가가 완만하게 변하는 지점(elbow) 선택 가능.\n",
    "    * 아래 그림: 100 차원 선택 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-10.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (MNIST 활용 예제) 압축을 위한 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* PCA를 MNIST 데이터셋의 차원축소를 위해 사용할 수 있음.\n",
    "* MINST 데이터셋의 주성분분석을 통해 95% 정도의 분산을 유지하려면 150개 정도의 주성분만 사용해도 됨.\n",
    "* 아래 코드: 154개 주성분 사용하여 차원축소하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 차원축소 결과:\n",
    "    * 784차원을 154 차원으로 줄임.\n",
    "    * 유실된 정보: 5%\n",
    "    * 크기: 원본 데이터셋 크기의 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 원본과의 비교: 정보손실 크지 않음 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-11.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 랜덤 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 주성분 선택을 위해 사용되는 SVD 알고리즘을 확률적으로 작동하도록 만드는 기법\n",
    "* 보다 빠르게 지정된 개수의 주성분에 대한 근삿값을 찾아줌.\n",
    "* $d$가 $n$ 보다 많이 작으면 기본 SVD 보다 훨씬 빠름.\n",
    "* 아래 코드: `svd_solver` 옵션을 `\"randomized\"`로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rnd_pca = PCA(n_components = 154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 점진적 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련세트를 미니배치로 나눈 후 IPCA(점진적 PCA)에 하나씩 주입 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 온라인 학습에 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 아래 코드: 넘파이의 `array_split()` 함수 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `partial_fit()` 활용에 주의할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 넘파이의 `memmap()` 클래스 활용 가능\n",
    "\n",
    "* 바이너리 파일로 저장된 (매우 큰) 데이터셋을 마치 메모리에 들어있는 것처럼 취급할 수 있는 도구 제공\n",
    "* 이를 이용하여 미니배치/온라인 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.4 커널 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 5장의 커널 트릭을 역으로 활용하는 기법.\n",
    "* 복잡한 비선형 사영 수행.\n",
    "* 사영된 후에 샘플의 군집을 유지하거나 꼬인 다양체 모양의 데이터셋을 펼칠 때 유용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 롤케이크에 적용한 다양한 커널 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-12.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 커널 선택과 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* kPCA는 비지도 학습\n",
    "* 비지도 학습은 명확한 성능 측정 기준 없음.\n",
    "* 하지만 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 방식 1: 전처리 용도로 사용 후 예측기와 연동하는 그리드탐색 등을 활용하여 성능 측정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression(solver=\"lbfgs\")) ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"] }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 방식 2: 가장 낮은 재구성 오차를 만드는 커널과 하이퍼파라미터 선택 가능\n",
    "* 단점: kPCA의 재구성은 선형 PCA 보다 훨씬 어려움. 아래 그림 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/ch08/homl08-13.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.5 LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 지역선형임베딩(LLE)은 비선형 차원축소 기법\n",
    "* 사영이 아닌 다양체학습에 의존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LLE 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정\n",
    "* 이후 국소적 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현 찾음.\n",
    "* 잡음이 많지 않은 다양체를 펼치는 데 잘 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예제: 롤케이크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/ch08/homl08-04.png\" width=\"300\"/> </td>\n",
    "        <td></td>\n",
    "        <td> <img src=\"images/ch08/homl08-14.png\" width=\"370\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8.6 다른 차원축소 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 랜덤 사영\n",
    "* 다차원 스케일링\n",
    "* Isomap\n",
    "* t-SNE\n",
    "* 선형 판별 분석"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "핸즈온머신러닝_7장.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
