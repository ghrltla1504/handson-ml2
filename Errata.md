핸즈온 머신러닝(2판) 번역 일부 수정
===

핸즈온 머신러닝(2판)에서 오해를 불러일으킬 수 있는 부분을 정리합니다.
간단한 오타는 [번역자의 블로그](https://tensorflow.blog/handson-ml2/)를 참조하세요.

## 7장

* 7장 전체: '앙상블 기법' 대신에 '앙상블 **기법**'으로 표현한다. 
    - 기법은 특별한 방법을 의미하기에 방법 보다는 기법이 적절해 보임.
    
* 245쪽, 위에서 셋째 줄
    - 번역 문장: ... 일련의 예측기(즉, 분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 ...
    - 수정 문장: ... (분류 또는 회귀 모델로 구성된) 예측기의 모음으로부터 수집된 예측을 종합하면 
        하나의 가장 좋은 모델보다 ...

* 245쪽, 위에서 넷째 줄
    - 번역 문장: 일연의 예측기를 앙상블이라고 부르기 때문에 ...
    - 수정 문장: 예측기의 모음을 앙상블이라고 부르기 때문에 ...

* 246쪽, 7.1절 제목
    - 번역 제목: 투표 기반 분류기
    - 수정 제목: 다수결 분류기
      (이유: 설명되는 방식이 기본적으로 '다수결'에 해당함.)
      
* 248쪽, 위에서 넷째 줄
    - 번역 문장: 하지만 이런 가정은 모든 분류기가 완벽하게 독립적이고 오차에 상관관계가 없어야 가능합니다.
    - 수정 문장: 하지만 이런 가정은 각 분류기에 의한 오차 사이에 아무런 상관관계가 없도록
        모든 분류기가 상호 독립적이어야 가능합니다. 

* 250쪽, 위에서 둘째 줄
    - 번역 단어: 수집 함수
    - 수정 단어: 종합 단어
        (이유: aggregation이 수집 결과를 이용하여 예측을 종합적으로 판단하기 때문)
        
* 252쪽, 7.2.2절 둘째 문단
    - 번역 문장: 앙상블의 평가는 각 예측기의 oob 평가를 평균하여 얻습니다.
    - 수정 문장: 앙상블 자체는 각 예측기의 oob 평가의 평균값을 이용하여 평가할 수 있다.

* 254쪽, 7.4절 첫째 문단, 둘째 줄
    - 원본/번역 문장: 전형적으로 max_samples를 훈련 세트의 크기로 지정합니다.
    - 훈련 세트의 크기는 max_samples 파라미터를 이용하여 지정합니다. 
    
    


## 6장
* 6장 전체: 아래 영어 단어들의 한글 표현을 통계학 분야에서 보다 일반적으로 사용되는 표현으로 바꿔 사용한다. 
    - 의사결정나무: 결정 트리(decision tree)
    - 뿌리 마디: 루트 노드(root node)
    - 마디: 노드(node)
    - 잎: 리프 노드(leaf node)
    
* 234쪽, 밑에서 둘째 줄
    - 번역 문장: ... 가장 순수한 서브셋으로 나눌 수 있는 ...
    - 수정 문장: ... 불순도가 가장 낮은 두 개의 부분집합으로 나날 수 있는 ...

* 237쪽, 아래에서 셋째 줄 (영어 원본 문장 역시 애미모호함.)
    - 번역 문장: (min_samples_leaf와 같지만 가중치가 부여된 전체 샘플 수에서의 비율)
    - 수정 문장: 샘플 별로 가중치가 있는 경우 가중치의 전체 합에서 해당 잎에 포함된 샘플의 가중치의 합에 차지하는 비율. 
        가중치가 설정되지 않았다면 모두 동일한 가중치를 가졌다고 가정함. 이런 경우 min_samples_leaf와 동일한 역할 수행.

* 237쪽, 아래에서 둘째 줄
    - 번역 문장: ... (각 노드에서 분할에 사용할 특성의 최대 수) ...
    - 수정 문장: ... (각 노드에서 분할 평가에 사용할 수 있는 특성의 최대 수) ...


## 5장

* 5장 전체: margin violation이 마진 오류로 번역되었음. 
    하지만 영어 표현(margin violation)에서 알 수 있듯이 
    오류 보다는 마진을 위반하는 경우를 뜻한다.
    따라서 여기서는 **마진 위반**으로 번역하는 것이 옳다고 보며, 따라서
    마진 위반을 강의노트에서 사용한다. 

* 207쪽: 그림 5-3 바로 아래 문단
    - 번역 문장: (즉, 샘플이 도로 중간이나 심지어 반대쪽에 있는 경우) ...
    - 수정 문장: (즉, 샘플이 도로 위나 심지어 도로 건너편에 있는 경우) ...
    
* 207쪽: 그림 5-4 바로 위 문장
    - 번역 문장: 하지만 이 경우에는 왼쪽 모델에 마진 오류가 많지만 일반화가 더 잘 될 것 같습니다.
    - 수정 문장: 그러나 왼쪽 모델의 경우 마진 위반이 더 발생하기는 하지만 일반화는 더 잘될 것입니다. 

## 4장

* 161쪽, 밑에서 둘째 줄
  - 번역 문장: 매우 비슷하지만 잡음 때문에 원래 함수의 파라미터를 정확하게 재현하지 못했습니다.
  - 수정 문장: 매우 비슷합니다. 어차피 잡음 때문에 원래 함수의 파라미터를 정확하게 재현할 수 없습니다. 

* 172쪽, 위에서 다섯째 줄
  - 번역 문장: 매 반복에서 다뤄야 할 데이터가 매우 적기 때문에 한 번에 하나의 샘플을 처리하면 알고리즘이 확실히 훨씬 빠릅니다.
  - 수정 문장: 하나의 샘플에 대해서만 계산하기 때문에 알고리즘이 확실히 훨씬 빠릅니다.

* 176쪽, 표 4-1, 맨 오른쪽 열
  - 원본 오류
    * 배치 경사 하강법: SGDRegressor 
    * 미니배치 경사하강법: SGDRegressor 
  - 오류 수정
    * 배치 경사 하강법: LogisticRegression
    * 미니배치 경사하강법: N/A (지원 없음을 의미함)

* 188쪽, 그림 4-18 바로 밑에서 둘째 줄
  - 원본 오류: ... 2차방정식처럼 보이며 ...
  - 수정 문장: ... 3차다항식 그래프처럼 보이며 ...

* 191쪽, 위에서 셋째 줄
  - 오류 문장: 릿지가 기본이 되지만 쓰이는 특성이 몇 개뿐이라고 ...
  - 수정 문장: 릿지가 기본이 되지만 유용한 특성이 몇 개뿐이라고 ...

* 197쪽, 위에서 첫째 줄
  - 오류 문장: 꽃잎의 너비를 기반으로 Iris-Versicolor 종을 감지하는 ... 
  - 수정 문장: 꽃잎의 너비를 기반으로 Iris-Virginica 종을 감지하는 ... 

## 3장

* 145쪽, 3.4절 바로 이전 문장
  - 번역 문장: 다음에는 숫자 5 이상을 감지해보겠습니다.
  - 수정 문장: 다음에는 5 이외의 숫자도 감지해보겠습니다.

* 145쪽, 3.4절 multiclass classifier 또는 classification 번역 문제
  - multiclass classifier를 다중 분류기, multiclass classification을 다중 분류로 번역하였음.
  - 문제점: 2장에서 multiple regression을 다중 회귀로 번역하였음. 그런데 multiclass의 의미는 multiple 과 엄연히 다름.
  - 일부 사람들은 multiclass를 다중 클래스 번역함. 이 점에 동의함.
  - 따라서 multiclass classifier는 다중 클래스 분류기 , multiclass classification은 다중 클래스 분류기를 사용하는 게 맞음.
  - 차라리 multiclass 대신에 multinomial을 사용해서 다항 분류기, 다항 분류 등으로 사용하면 혼란을 피할 수 있음.

* 145쪽, 3.4절 첫 문단
  - 번역 문장: ... 둘 이상의 클래스를 구별할 수 있습니다.
  - 수정 문장: ... 셋 이상의 클래스를 구별할 수 있습니다.

* 146쪽, 중간 "간단하네요!" 로 시작하는 문단
  - 번역 문장: 내부에서는 사이킷런이 OvO 전략을 사용해 10개의 이진 분류기를 훈련시키고 각각의 결정 점수를 얻어 점수가 가장 높은 클래스를 선택합니다.
  - 수정 문장: 내부에서는 사이킷런이 OvO 전략을 사용해 45개의 이진 분류기를 훈련시키고 가장 많은 결투를 이긴 클래스를 선택합니다.

* 148쪽, 중간에 위치한 코드 바로 위 문단
  - 번역 문장: 예를 들어 간단하게 입력의 스케일을 조정하면 (2장에서처럼) 정확도를 89% 이상으로 높일 수 있습니다.
  - 수정 문장: 예를 들어 (2장에서처럼) 간단하게 입력의 스케일을 조정하면 정확도를 89% 이상으로 높일 수 있습니다.

* 153쪽, 상단 코드 바로 아래 문단
  - 번역 문장: 실제로는 아닐 수 있지만 이 코드는 모든 레이블의 가중치가 같다고 가정한 것입니다. 특히 앨리스 사진이 밥이나 찰리 사진보다 훨씬 많다면 앨리스 사진에 대한 분류기의 점수에 더 높은 가중치를 둘 것입니다.
  - 수정 문장: 모든 레이블의 중요도가 같다가 가정한 결과입니다. 물론 실제로는 그렇지 않을 수 있습니다. 예를 들어, 앨리스 사진이 밥이나 찰리 사진보다 훨씬 많다면 앨리스 사진에 대한 분류기의 점수에 더 높은 가중치를 두고자 할 수도 있을 것입니다.

* 153쪽, 하단 NOTE_ 바로 위 문단
  - 번역 문장: ... 각 레이블은 값을 여러 개 가집니다(0부터 255까지 픽셀 강도).
  - 수정 문장: ... 각 레이블은 값을 여러 개 가집니다(픽셀 강도는 0부터 255까지임).

## 2장

* 107쪽, 맨 아래 CAUTION_ 문단
  - 번역 문장: 해당 문단 전체
  - 수정 문장: (번역 수정과 함께 부가 설명 좀 더 추가됨)

    모든 변환기의 `fit()` 메서드는 훈련 데이터에 대해서만 적용한다. 
    두 스케일링 변환기 `MinMaxScaler`와 `StandardScaler`의 경우 `fit()` 메서드는
    아래 값들을 계산해야 한다.

    * `MinMaxScaler`의 `fit()` 메서드: 특성별 최소값과 최대값
    * `StandardScaler`의 `fit()` 메서드: 특성별 평균값과 표준편차

    반면에 `transform()` 메서드는 모든 데이터에 대해 적용한다. 
    즉, 훈련 세트를 이용하여 필요한 파라미터를 확인한 후 그 값들을 이용하여 전체 데이터셋트를 변환한다.
    예를 들어, 따로 떼어놓은 테스트 데이터들은 훈련 데이터를 이용하여 확인된 값들을 이용하여 
    특성 스케일링을 진행한다.

* 108쪽, num_pipeline 정의 바로 아래 문단
  - 번역 문장: 마지막 단계에는 변환기와 추정기를 모두 사용할 수 있고 그 외에는 모두 변환기여야 합니다.
  - 수정 문장: 마지막 단계 이외에는 모두 변환기여야 합니다.